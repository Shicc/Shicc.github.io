<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GeekShi</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://shic.top/"/>
  <updated>2019-05-27T03:09:09.373Z</updated>
  <id>http://shic.top/</id>
  
  <author>
    <name>this.Shi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>neural-style</title>
    <link href="http://shic.top/Deep-Learning/neural-style/"/>
    <id>http://shic.top/Deep-Learning/neural-style/</id>
    <published>2019-05-26T16:00:00.000Z</published>
    <updated>2019-05-27T03:09:09.373Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经网络-样式迁移"><a href="#神经网络-样式迁移" class="headerlink" title="神经网络 样式迁移"></a>神经网络 样式迁移</h1><p>计算机视觉当中，一个比较有趣的模型就是样式迁移了。<br><img src="https://github.com/Shicc/webrecourse/raw/master/img/s-t.png" alt="style-transfer"><br><a id="more"></a><br>二维卷积层输出的二维数组，是输入在空间维度（宽和高）上的某一级的表征，也叫特征图（feature map）。设特征图上某一个位置的元素为x，那么影响x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做 x 的感受野（receptive field）。随着卷积层的加深，得到的特征图越来越小且越来越厚，特征图上的一个单位元素所对应原始图片上的区域就会更大，也就是其感受野更大，更加容易捕捉到更大尺寸的特征，而靠近输入的一些卷积层所输出的特征图上的元素的单位元素的感受野就很小，更适合提取图像的细节信息。</p>
<p>例如下图，在①处和在②处，相同大小的特征块在原始图像上的感受野是不同的。所以我们可以利用这个特性，通过深度卷积神经网络的不同层，来提取我们需要的信息。<br><img src="https://github.com/Shicc/webrecourse/raw/master/img/feature-map.jpg" alt="freature map"></p>
<h2 id="内容和样式图像"><a href="#内容和样式图像" class="headerlink" title="内容和样式图像"></a>内容和样式图像</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">d2l.set_figsize()</div><div class="line">content_img = image.imread(<span class="string">'/style-trans-img/2.jpg'</span>) <span class="comment"># 内容图像位置</span></div><div class="line">d2l.plt.imshow(content_img.asnumpy())</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">style_img = image.imread(<span class="string">'/style-trans-img/star.jpg'</span>) <span class="comment"># 样式图像位置</span></div><div class="line">d2l.plt.imshow(style_img.asnumpy());</div></pre></td></tr></table></figure>
<p>定义图像预处理函数对输入的图像在RGB三个通道分别做标准化，并将转化数据格式，把通道数放在第一维度，变成mxnet里的卷积神经网络的通用输入格式。后处理函数是把合成图像的神经网络的输出变回标准化之前的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">rgb_mean = nd.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]) <span class="comment"># ImageNet上的图片的RGB均值</span></div><div class="line">rgb_std = nd.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># ImageNet上的图片的RGB标准差</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(img, image_shape)</span>:</span></div><div class="line">    img = image.imresize(img, *image_shape)</div><div class="line">    img = (img.astype(<span class="string">'float32'</span>) / <span class="number">255</span> - rgb_mean) / rgb_std</div><div class="line">    <span class="keyword">return</span> img.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).expand_dims(axis=<span class="number">0</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess</span><span class="params">(img)</span>:</span></div><div class="line">    img = img[<span class="number">0</span>].as_in_context(rgb_std.context)</div><div class="line">    <span class="keyword">return</span> (img.transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)) * rgb_std + rgb_mean).clip(<span class="number">0</span>, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>在预训练的深度卷积神经网络中，选择靠近输出的层，也称内容层，来输出图像的内容特征。选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层。先引入所有需要使用的模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, gluon, image, init, nd</div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> model_zoo, nn</div><div class="line"><span class="keyword">import</span> time</div></pre></td></tr></table></figure>
<h3 id="抽取特征"><a href="#抽取特征" class="headerlink" title="抽取特征"></a>抽取特征</h3><p>我们可以选择在ImageNet上训练好的VGG，ResNet等网络中的某些层来抽取图像的内容特征和样式特征。原论文中选择了VGG19，在VGG网络中，使用了5个卷积块。我们可以随意选择一些层，可以得到不同的效果。以下选择会得到不错的效果，内容层选择第四卷积块的最后一层，样式层选择每一个卷积块的第一个卷积层，这样可以抽取到局部和全局的样式。以下这些层就是提取内容和样式的层，可以把这些层来构建一个新的网络，用这个新网络来提取图像特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">style_layers, content_layers = [<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">19</span>, <span class="number">28</span>], [<span class="number">25</span>]</div><div class="line"></div><div class="line">net = nn.Sequential()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(max(content_layers + style_layers) + <span class="number">1</span>):</div><div class="line">    net.add(pretrained_net.features[i])</div></pre></td></tr></table></figure>
<p>但一个网络的前向计算，只会得到最后一层的输出，因此在提取不同的特征时，需要逐层提取，以下函数返回的是通过特征提取网络提取到的输入X的内容特征图和样式特征图。（注：此处的输入X应为标准化后的适合net的输入格式的多维数组）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(img, image_shape, content_layers, style_layers)</span>:</span></div><div class="line">    contents = []</div><div class="line">    styles = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(net)):</div><div class="line">        X = net[i](X)</div><div class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> style_layers:</div><div class="line">            styles.append(X)</div><div class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> content_layers:</div><div class="line">            contents.append(X)</div><div class="line">    <span class="keyword">return</span> contents, styles <span class="comment"># 返回图像的内容特征和样式特征</span></div></pre></td></tr></table></figure>
<p>到此，我们通过预训练的VGG的某些层，便可以计算得到输入X的样式和内容了。如果上面函数输入是内容图片，那么我们所需要的就只是返回的<code>contents</code>值，如果输入的是样式图片，那么值需要函数返回的 <code>styles</code>值。提取特征的层的参数都不需要更新，唯一需要更新的就是合成图像的模型参数了。所以在训练合成模型之前，我们就可以先获得我们所需的内容和样式特征，定义下面两个函数分别获取内容图片的内容特征和样式图片的样式特征。这样做有几个好处：</p>
<ol>
<li>逻辑更加清晰，提取内容的函数返回内容相关，提取样式的返回和样式相关，虽然它们内部都是调用预处理函数和特征提取函数。</li>
<li>后续初始化合成模型时是以内容图片为基准的，显然神经网络也需要预处理的内容图片，这样可以直接把返回的<code>content_img_norm</code>传过去而不必再调用预处理函数也不必再传合成图片所需要的大小。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_contents</span><span class="params">(image_shape, ctx)</span>:</span></div><div class="line">    <span class="comment"># 返回标准化的内容图像和内容图片的内容特征</span></div><div class="line">    content_img_norm = preprocess(content_img, image_shape).copyto(ctx)</div><div class="line">    content_img_contents, _ = extract_features(content_X, content_layers, style_layers)</div><div class="line">    <span class="keyword">return</span> content_img_norm, content_img_contents</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_styles</span><span class="params">(image_shape, ctx)</span>:</span></div><div class="line">    <span class="comment"># 返回标准化的样式图像和样式图片的样式特征，但前者用不到</span></div><div class="line">    style_img_norm = preprocess(style_img, image_shape).copyto(ctx)</div><div class="line">    _, style_img_styles = extract_features(style_X, content_layers, style_layers)</div><div class="line">    <span class="keyword">return</span> style_img_norm, style_img_styles</div></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>内容损失的两个输入时合成图像的内容特征和内容图像的内容特征，通过平方误差函数衡量它们的差异</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss</span><span class="params">(Y_hat, Y)</span>:</span></div><div class="line">    <span class="keyword">return</span> (Y_hat - Y).square().mean()</div></pre></td></tr></table></figure>
<p>样式损失中需要格拉姆矩阵运算，先定义格拉姆函数，style_loss也是通过平方误差函数衡量两个格拉姆矩阵之间的差异。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 格拉姆矩阵</span></div><div class="line"><span class="comment"># X是一个（batch,channel,h,w）</span></div><div class="line"><span class="comment"># n是计算得到的batch*h*w</span></div><div class="line"><span class="comment"># 再把这个矩阵重塑形状为(channels, n)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram</span><span class="params">(X)</span>:</span></div><div class="line">    num_channels, n = X.shape[<span class="number">1</span>], X.size // X.shape[<span class="number">1</span>]</div><div class="line">    X = X.reshape((num_channels, n))</div><div class="line">    <span class="keyword">return</span> nd.dot(X, X.T) / (num_channels * n)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss</span><span class="params">(Y_hat, gram_Y)</span>:</span></div><div class="line">    <span class="keyword">return</span> (gram(Y_hat) - gram_Y).square().mean()</div></pre></td></tr></table></figure>
<p>总变差损失是一个降噪算法，它使得图像中的像素尽可能和邻近的像素值相似，输入值为合成图像的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tv_loss</span><span class="params">(Y_hat)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * ((Y_hat[:, :, <span class="number">1</span>:, :] - Y_hat[:, :, :<span class="number">-1</span>, :]).abs().mean() +</div><div class="line">                  (Y_hat[:, :, :, <span class="number">1</span>:] - Y_hat[:, :, :, :<span class="number">-1</span>]).abs().mean())</div></pre></td></tr></table></figure>
<p>损失函数，计算各类损失和总的损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">style_channels = [net[l].weight.shape[<span class="number">0</span>] <span class="keyword">for</span> l <span class="keyword">in</span> style_layers] <span class="comment"># net某层的通道数</span></div><div class="line"><span class="comment"># 调节这三个参数可以权衡合成图像在保留内容、迁移样式以及降噪。</span></div><div class="line"><span class="comment"># content_weight, style_weight = 10, 10 的话就基本和原图无区别</span></div><div class="line"><span class="comment"># 将content_weight调小，style_weight调大，画面才很风格化</span></div><div class="line">content_weight = <span class="number">1</span></div><div class="line">style_weights  = [<span class="number">1e5</span>//n <span class="keyword">for</span> n <span class="keyword">in</span> style_channels] <span class="comment"># 尽量提取到细节的样式而弱化全局样式</span></div><div class="line">tv_weight = <span class="number">10</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(X, contents_Y_hat, styles_Y_hat, content_img_contents, styles_gram)</span>:</span></div><div class="line">    <span class="comment"># 分别计算内容损失、样式损失和总变差损失</span></div><div class="line">    contents_l = [content_loss(Y_hat, Y) * content_weight <span class="keyword">for</span> Y_hat, Y <span class="keyword">in</span> zip(</div><div class="line">        contents_Y_hat, content_img_contents)]</div><div class="line">    styles_l = [style_loss(Y_hat, Y) * style_weight <span class="keyword">for</span> style_weight, Y_hat, Y <span class="keyword">in</span> zip(</div><div class="line">        style_weights, styles_Y_hat, styles_Y_gram)]</div><div class="line">    tv_l = tv_loss(X) * tv_weight</div><div class="line">    <span class="comment"># 对所有损失求和</span></div><div class="line">    l = nd.add_n(*styles_l) + nd.add_n(*contents_l) + tv_l</div><div class="line">    <span class="keyword">return</span> contents_l, styles_l, tv_l, l</div></pre></td></tr></table></figure>
<h3 id="初始化合成图像，这是唯一需要训练的模型。前向计算就是返回其权重参数，即合成的图片。"><a href="#初始化合成图像，这是唯一需要训练的模型。前向计算就是返回其权重参数，即合成的图片。" class="headerlink" title="初始化合成图像，这是唯一需要训练的模型。前向计算就是返回其权重参数，即合成的图片。"></a>初始化合成图像，这是唯一需要训练的模型。前向计算就是返回其权重参数，即合成的图片。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeneratedImage</span><span class="params">(nn.Block)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_shape, **kwargs)</span>:</span></div><div class="line">        super(GeneratedImage, self).__init__(**kwargs)</div><div class="line">        self.weight = self.params.get(<span class="string">'weight'</span>, shape=img_shape)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.weight.data()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_inits</span><span class="params">(X, ctx, lr, styles)</span>:</span></div><div class="line">    gen_img = GeneratedImage(X.shape)</div><div class="line">    gen_img.initialize(init.Constant(X), ctx=ctx, force_reinit=<span class="keyword">True</span>)</div><div class="line">    trainer = gluon.Trainer(gen_img.collect_params(), <span class="string">'adam'</span>,</div><div class="line">                            &#123;<span class="string">'learning_rate'</span>: lr&#125;)</div><div class="line">    styles_gram = [gram(Y) <span class="keyword">for</span> Y <span class="keyword">in</span> styles]</div><div class="line">    <span class="keyword">return</span> gen_img(), styles_gram, trainer</div></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(content_img_norm, content_img_contents, stystyle_img_styles, lr, max_epochs, lr_decay_epoch)</span>:</span></div><div class="line">	<span class="string">'''</span></div><div class="line"><span class="string">	参数:</span></div><div class="line"><span class="string">		content_img_norm:标准化的内容图像，用作初始化合成图像，这样可以加速训练</span></div><div class="line"><span class="string">		content_img_contents:内容图像的内容</span></div><div class="line"><span class="string">		stystyle_img_styles:样式图像的样式</span></div><div class="line"><span class="string">		lr:学习率</span></div><div class="line"><span class="string">		max_epochs:最大迭代周期</span></div><div class="line"><span class="string">		lr_decay_epoch:学习率衰减周期</span></div><div class="line"><span class="string">	返回：</span></div><div class="line"><span class="string">		X：合成图像的输出（即一个前向计算）</span></div><div class="line"><span class="string">	'''</span></div><div class="line">	ctx = d2l.try_gpu()</div><div class="line">    X, styles_gram, trainer = get_inits(content_img_norm, ctx, lr, stystyle_img_styles)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_epochs):</div><div class="line">        start = time.time()</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">            contents_Y_hat, styles_Y_hat = extract_features(</div><div class="line">                X, content_layers, style_layers)</div><div class="line">            contents_l, styles_l, tv_l, l = compute_loss(</div><div class="line">                X, contents_Y_hat, styles_Y_hat, content_img_contents, styles_gram)</div><div class="line">        l.backward()</div><div class="line">        trainer.step(<span class="number">1</span>)</div><div class="line">        nd.waitall() <span class="comment"># @有空解释一下</span></div><div class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span> <span class="keyword">and</span> i != <span class="number">0</span>:</div><div class="line">            print(<span class="string">'epoch %3d, content loss %.2f, style loss %.2f, '</span></div><div class="line">                  <span class="string">'TV loss %.2f, %.2f sec'</span></div><div class="line">                  % (i, nd.add_n(*contents_l).asscalar(), <span class="comment"># @有空解释一下</span></div><div class="line">                     nd.add_n(*styles_l).asscalar(), tv_l.asscalar(),</div><div class="line">                     time.time() - start)) <span class="comment"># 这里的时间只是一个周期的</span></div><div class="line">        <span class="keyword">if</span> i % lr_decay_epoch == <span class="number">0</span> <span class="keyword">and</span> i != <span class="number">0</span>:</div><div class="line">            <span class="comment"># 减少学习率</span></div><div class="line">            trainer.set_learning_rate(trainer.learning_rate * <span class="number">0.1</span>)</div><div class="line">            print(<span class="string">'change lr to %.1e'</span> % trainer.learning_rate)</div><div class="line">    <span class="keyword">return</span> X</div></pre></td></tr></table></figure>
<p>设置需要的合成图片大小，爆显存就调小一点。此大小2GB显存够用，设置好并开始训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">image_shape = (<span class="number">400</span>, <span class="number">300</span>)</div><div class="line">net.collect_params().reset_ctx(ctx)</div><div class="line"></div><div class="line">content_img_norm, content_img_contents = get_contents(image_shape, ctx)</div><div class="line">_, style_img_styles = get_styles(image_shape, ctx)</div><div class="line"><span class="comment"># 内容图片_norm,内容图片_norm的内容,样式图片的样式</span></div><div class="line">output = train(content_img_norm, content_img_contents, stystyle_img_styles, <span class="number">0.01</span>, <span class="number">500</span>, <span class="number">200</span>)</div></pre></td></tr></table></figure>
<p>保存图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">d2l.plt.imsave(<span class="string">'../img/neural-style-1.png'</span>, postprocess(output).asnumpy())</div></pre></td></tr></table></figure>
<p>其实看效果还是可以。想让合成图像更加接近样式图像，可以调大一些样式的权重，但注意如果所有样式层都有很大的权重那么可能会学习到更多的全局样式信息。不同参数下的效果也不同，总之这是一个很有趣的模型。<br><img src="https://github.com/Shicc/webrecourse/raw/master/img/s-t1.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;神经网络-样式迁移&quot;&gt;&lt;a href=&quot;#神经网络-样式迁移&quot; class=&quot;headerlink&quot; title=&quot;神经网络 样式迁移&quot;&gt;&lt;/a&gt;神经网络 样式迁移&lt;/h1&gt;&lt;p&gt;计算机视觉当中，一个比较有趣的模型就是样式迁移了。&lt;br&gt;&lt;img src=&quot;https://github.com/Shicc/webrecourse/raw/master/img/s-t.png&quot; alt=&quot;style-transfer&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Object Detection with SSD</title>
    <link href="http://shic.top/Deep-Learning/od-ssd/"/>
    <id>http://shic.top/Deep-Learning/od-ssd/</id>
    <published>2019-05-23T16:00:00.000Z</published>
    <updated>2019-05-24T08:56:30.209Z</updated>
    
    <content type="html"><![CDATA[<h1 id="单发多框检测"><a href="#单发多框检测" class="headerlink" title="单发多框检测"></a>单发多框检测</h1><p>在目标检查的任务中，单发多框检测（single shot multibox detection，SSD）是一个计算量和性能都居中的算法，容易理解且实现不算复杂，广泛应用于很多检测模型中。<br><a id="more"></a></p>
<h2 id="SSD网络"><a href="#SSD网络" class="headerlink" title="SSD网络"></a>SSD网络</h2><p>正如名字描述那样，SSD由一个提取图像特征的基础卷积网络和若干个多尺度的特征块串联而成。首先由基础网络提取图像的特征，输出其特征图，再由一个锚框生成算法（@后续补充）得到该特征图上的锚框，再针对每一个锚框做类别和边界框偏移量的预测。</p>
<p>假设有c类，那么每一个锚框的类别就是c+1类。在基础网络块后面分别串联上若干个长宽减半的卷积层，逐渐缩小特征图的大小，每一个长宽减半卷积层的输出上又会和前面一样生成很多个锚框，再分别做对类别和边界框的预测，该缩小后的特征块又是下一个长宽减半的卷积层的输入。这样一来，位于网络前面的特征块较大，特征块中每一个单元的感受野较小，所以适合够检测出一下较小的目标，而越往后，特征块越来越小，其每个单元的感受野越来越大，就适合检测较大的目标。</p>
<p><a href="https://gluon-cv.mxnet.io/" target="_blank" rel="external">gluoncv</a>的<code>model_zoo</code>里提供了很多可用的模型，可以选择是否加载预训练的权重，也可以重新设置模型的输出类别或者设置自己需要的权重，这一点是十分的方便的。我们首先通过下面一行代码得到我们需要的网络，具体有哪些网络可选可以参考<a href="https://gluon-cv.mxnet.io/model_zoo/index.html" target="_blank" rel="external">官网</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net = model_zoo.get_model(<span class="string">'ssd_512_resnet50_v1_voc'</span>, pretrained_base = <span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p><code>pretrained_base = False</code>表示不加载预训练的权重，返回的<code>net</code>只是一个空的框架且，设置<code>True</code>则会下载该模型的预训练的权重参数，如果使用<code>pretrained=True/False</code>则都会下载权重，只是设置是否加载权重到模型。</p>
<p>比如说之前我接触到一个项目是训练一个能定位乌龟在哪里，这个乌龟是怎么回事呢？是工作室里其他小伙伴的智能孵化项目孵化出来的乌龟，我们需要隔几小时就要对它拍一张照片然后定位它在培养箱的哪个位置。如果远离了乌龟灯的光照区就发出一条警告给我们。那么我们在这个项目里面只需要检测一个类，即使是乌龟类别不在其预训练的类别中，也可以通过一行代码进行设置。当然预训练的网络中基础网络模块基本都是基于<code>ImageNet</code>训练的分类网络，也可以直接用到自己项目中，毕竟提取特征的权重参数还是很有用的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">my_net = model_zoo.get_model(<span class="string">'ssd_512_resnet50_v1_voc'</span>, pretrained_base = <span class="keyword">True</span>)</div><div class="line">my_net.reset_class(classes=[<span class="string">'tortoise'</span>])</div></pre></td></tr></table></figure>
<h2 id="Pascal-VOC2012数据集"><a href="#Pascal-VOC2012数据集" class="headerlink" title="Pascal VOC2012数据集"></a>Pascal VOC2012数据集</h2><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h3><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank" rel="external">Pascal VOC2012数据集</a>是一个用于目标检测和语义分割的著名数据集，是VOC2007的续集，VOC数据集系列的文件结构如下：</p>
<ul>
<li>VOCdevkit<ul>
<li>VOC2007<ul>
<li>…</li>
</ul>
</li>
<li>VOC2012<ul>
<li>Annotations：存放目标检测的标签</li>
<li>ImageSets：存放各种用途文件的名字</li>
<li>JPEGImages：图片文件</li>
<li>SegmentationClass：语义分割的标注</li>
<li>SegmentationObject：实例分割的标注</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>在官网下载好数据集解压后，我们可以按照自己的任务需求来使用数据集，其中VOC2012\ImageSets\Main里面就有目标检测所需要的文件名字信息。mxnet的计算机视觉工具包gluoncv提供了一个直接读取VOC目标检测样本的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> gluoncv.data <span class="keyword">import</span> VOCDetection</div><div class="line"><span class="comment"># root为文件路径，splits指定的是VOC的类别和需要用到的文件名</span></div><div class="line">train_dataset = VOCDetection(root = <span class="string">'../data/VOCdevkit'</span>,splits=[(<span class="number">2012</span>, <span class="string">'trainval'</span>)])</div><div class="line">print(<span class="string">'Training images:'</span>, len(train_dataset))</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>Training images: <span class="number">11540</span></div></pre></td></tr></table></figure>
<p><code>train_dataset</code>是一个<code>gluoncv.data.pascal_voc.detection.VOCDetection</code>的类对象，可以通过<code>__getitem__(idx)</code>或者<code>[idx]</code>访问第<code>idx</code>个元素，其中前面的大矩阵中存放的是图片信息，后面较小矩阵中有几行就有几个类，一行中前四个是边界框的值，第五个是该框中的类别，第六个值暂不知是什么</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">train_dataset.__getitem__(<span class="number">0</span>)</div><div class="line"></div><div class="line">&gt;&gt;&gt;</div><div class="line">(</div><div class="line"> [[[<span class="number">255</span> <span class="number">255</span> <span class="number">255</span>]</div><div class="line">   [<span class="number">255</span> <span class="number">255</span> <span class="number">255</span>]</div><div class="line">   [<span class="number">255</span> <span class="number">255</span> <span class="number">255</span>]</div><div class="line">   ...</div><div class="line">   [<span class="number">202</span> <span class="number">205</span> <span class="number">248</span>]</div><div class="line">   [<span class="number">203</span> <span class="number">206</span> <span class="number">249</span>]</div><div class="line">   [<span class="number">204</span> <span class="number">207</span> <span class="number">250</span>]]</div><div class="line">   ...</div><div class="line">   &lt;NDArray <span class="number">442</span>x500x3 @cpu(<span class="number">0</span>)&gt;,</div><div class="line">   array(</div><div class="line">       [[ <span class="number">52.</span>,  <span class="number">86.</span>, <span class="number">470.</span>, <span class="number">419.</span>,  <span class="number">12.</span>,   <span class="number">0.</span>],</div><div class="line">        [<span class="number">157.</span>,  <span class="number">43.</span>, <span class="number">288.</span>, <span class="number">166.</span>,  <span class="number">14.</span>,   <span class="number">0.</span>]]</div><div class="line">        )</div><div class="line">)</div></pre></td></tr></table></figure>
<p>打印出该图片看看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">img0, label0 = train_dataset.__getitem__(<span class="number">0</span>)</div><div class="line">bboxes = label0[:,:<span class="number">4</span>] <span class="comment"># 也就是上面的52.,  86., 470., 419和157.,  43., 288., 166.</span></div><div class="line">cids = label0[:,<span class="number">4</span>:<span class="number">5</span>] <span class="comment"># 就是上面的类别12和14</span></div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> gluoncv.utils <span class="keyword">import</span> viz</div><div class="line"></div><div class="line">ax = viz.plot_bbox(</div><div class="line">    img0.asnumpy(),</div><div class="line">    bboxes,</div><div class="line">    labels=cids,</div><div class="line">    class_names=train_dataset.classes)</div><div class="line">plt.show()</div><div class="line"></div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p><img src="https://github.com/Shicc/webrecourse/raw/master/img/voc_dect%5B0%5D.png" alt="train_dataset[0]"></p>
<h3 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h3><p>先引入后续需要的模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> gluoncv <span class="keyword">import</span> model_zoo</div><div class="line"><span class="keyword">from</span> gluoncv.data.transforms <span class="keyword">import</span> presets</div><div class="line"><span class="keyword">from</span> gluoncv <span class="keyword">import</span> utils</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd, gluon, autograd, init</div><div class="line"><span class="keyword">from</span> gluoncv.data.batchify <span class="keyword">import</span> Tuple, Stack</div><div class="line"><span class="keyword">from</span> mxnet.gluon.data <span class="keyword">import</span> DataLoader</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"><span class="keyword">from</span> gluoncv.loss <span class="keyword">import</span> SSDMultiBoxLoss</div><div class="line"><span class="keyword">import</span> time</div></pre></td></tr></table></figure>
<p>gluoncv提供了数据转换和图像增广的类<code>SSDDefaultTrainTransform</code>，在该类的<code>__init__</code>函数就会把图像标准化。该类是一个数据转换的类，规定的是一个怎么做数据转化的规则，传入两个int参数<code>width, height</code>就会把图像转化成<code>(batch_size, channels, 512, 512)</code>, （@忘记查看_label0了，之后补上）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">width, height = <span class="number">512</span>, <span class="number">512</span>  <span class="comment"># 设置训练图片的长宽</span></div><div class="line"></div><div class="line">train_transform0 = presets.ssd.SSDDefaultTrainTransform(width, height)</div><div class="line">_img0, _label0 = train_transform(img0, label0)</div><div class="line">print(<span class="string">'tensor shape:'</span>, _img0.shape) <span class="comment"># 这里传入的一张图片，故没有batch_size</span></div><div class="line">print(<span class="string">'原来：'</span>, img0.shape)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>tensor shape: (<span class="number">3</span>, <span class="number">512</span>, <span class="number">512</span>)</div><div class="line">    原来： (<span class="number">442</span>, <span class="number">500</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure>
<p>而<code>_img0</code>已经是被标准化的数据了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">print(_img0)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[[[ <span class="number">2.1276248</span>   <span class="number">2.1440527</span>   <span class="number">2.1421344</span>  ...  <span class="number">1.4059145</span>   <span class="number">1.3767488</span></div><div class="line">    <span class="number">1.3458692</span> ]</div><div class="line">    [ <span class="number">2.1166914</span>   <span class="number">2.1349118</span>   <span class="number">2.143327</span>   ...  <span class="number">1.3894821</span>   <span class="number">1.3782884</span></div><div class="line">    <span class="number">1.3341411</span> ]</div><div class="line">    [ <span class="number">2.1036239</span>   <span class="number">2.1237183</span>   <span class="number">2.1413574</span>  ...  <span class="number">1.3673979</span>   <span class="number">1.3768137</span></div><div class="line">    <span class="number">1.3274454</span> ]</div><div class="line">    ...]]]</div><div class="line">    &lt;NDArray <span class="number">3</span>x512x512 @cpu(<span class="number">0</span>)&gt;</div></pre></td></tr></table></figure>
<h3 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h3><p>在SSD的默认数据转化的类<code>SSDDefaultTrainTransform</code>中，有一个<code>anchors</code>的参数可选，上面是没有该参数的情况，如果加上该参数，可以再数据转化中生成训练目标从而加速训练过程，意思是在数据转化的时候就通过CPU生成好锚框用于检测，而不用等到在GPU上训练的时候再去选框检测。根据源代码中的提示：<code>Anchors generated from SSD networks, the shape must be (1, N, 4)</code>,即这个SSD数据转化类的<code>anchors</code>需要用SSD网络来生成，这里的SSD是基于mxnet的<code>HybridBlock</code>，在预测状态下输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cids, scores, bboxes = net(x) <span class="comment"># 类标签，预测置信分数，边界框</span></div></pre></td></tr></table></figure>
<p>而在训练模式下的输出则不用，<code>cls_preds</code>是<code>softmax</code>的类预测， <code>box_preds</code>是与锚点一一对应的边界框偏移，<code>anchors</code>是相应锚点框的绝对坐标，通过下面的语句，我们便获取了我们需要的<code>anchors</code>的值了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> autograd.train_mode():</div><div class="line">    cls_preds, box_preds, anchors = net(x)</div></pre></td></tr></table></figure>
<p>有别于前面的数据加载，现在的加载器里传入的<code>transform</code>函数不同了。（@有空解释一下批量处理的函数<code>batchify_fn</code>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">train_transform = presets.ssd.SSDDefaultTrainTransform(width, height, anchors)</div><div class="line">batchify_fn = Tuple(Stack(), Stack(), Stack())</div><div class="line">train_loader = DataLoader(</div><div class="line">    train_dataset.transform(train_transform),</div><div class="line">    batch_size, <span class="comment"># 记得提前定义好batch_size和num_workers</span></div><div class="line">    shuffle=<span class="keyword">True</span>,</div><div class="line">    batchify_fn=batchify_fn,</div><div class="line">    last_batch=<span class="string">'rollover'</span>,</div><div class="line">    num_workers=num_workers)</div><div class="line"></div><div class="line"><span class="keyword">for</span> ib, batch <span class="keyword">in</span> enumerate(train_loader):</div><div class="line">    <span class="keyword">if</span> ib &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># 这里得到的batch就是批数据</span></div><div class="line">    <span class="comment"># batch[0]是数据本身(batch_size,channels,width, height)</span></div><div class="line">    <span class="comment"># batch[1]是要预测的类别(batch_size,框数),框数就是网络输出的anchors数</span></div><div class="line">    <span class="comment"># batch[2]是要预测的框(batch_size,框数,4)</span></div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>mxnet都提供了很好用的api，使得本来复杂的目标检测loss很好编写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">mbox_loss = SSDMultiBoxLoss()</div><div class="line">ce_metric = mx.metric.Loss(<span class="string">'CrossEntropy'</span>)</div><div class="line">smoothl1_metric = mx.metric.Loss(<span class="string">'SmoothL1'</span>)</div><div class="line"></div><div class="line">ctx = [mx.gpu()]</div><div class="line">net.initialize(init=init.Xavier(), ctx = ctx, force_reinit=<span class="keyword">True</span>) <span class="comment"># force_reinit=True 表示强制初始化</span></div><div class="line"></div><div class="line">trainer = gluon.Trainer(</div><div class="line">    net.collect_params(), <span class="string">'sgd'</span>,</div><div class="line">    &#123;<span class="string">'learning_rate'</span>: <span class="number">0.001</span>, <span class="string">'wd'</span>: <span class="number">0.0005</span>, <span class="string">'momentum'</span>: <span class="number">0.9</span>&#125;)</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">40</span>):</div><div class="line">    ce_metric.reset()</div><div class="line">    smoothl1_metric.reset()</div><div class="line">    net.hybridize(static_alloc=<span class="keyword">True</span>, static_shape=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">for</span> ib, batch <span class="keyword">in</span> enumerate(train_loader):</div><div class="line">        batch_size = batch[<span class="number">0</span>].shape[<span class="number">0</span>]</div><div class="line">        <span class="comment"># 通过访问batch[idx]来获取三种数据</span></div><div class="line">        data = gluon.utils.split_and_load(batch[<span class="number">0</span>], ctx_list=ctx, batch_axis=<span class="number">0</span>)</div><div class="line">        cls_targets = gluon.utils.split_and_load(batch[<span class="number">1</span>], ctx_list=ctx, batch_axis=<span class="number">0</span>)</div><div class="line">        box_targets = gluon.utils.split_and_load(batch[<span class="number">2</span>], ctx_list=ctx, batch_axis=<span class="number">0</span>)</div><div class="line">        <span class="keyword">with</span> autograd.record():</div><div class="line">           cls_preds = []</div><div class="line">           box_preds = []</div><div class="line">           <span class="keyword">for</span> x <span class="keyword">in</span> data:</div><div class="line">               cls_pred, box_pred, _ = net(x)</div><div class="line">               cls_preds.append(cls_pred)</div><div class="line">               box_preds.append(box_pred)</div><div class="line">           sum_loss, cls_loss, box_loss = mbox_loss(</div><div class="line">                    cls_preds, box_preds, cls_targets, box_targets)</div><div class="line">           autograd.backward(sum_loss)</div><div class="line">        trainer.step(<span class="number">1</span>)</div><div class="line">        ce_metric.update(<span class="number">0</span>, [l * batch_size <span class="keyword">for</span> l <span class="keyword">in</span> cls_loss])</div><div class="line">        smoothl1_metric.update(<span class="number">0</span>, [l * batch_size <span class="keyword">for</span> l <span class="keyword">in</span> box_loss])</div><div class="line">        name1, loss1 = ce_metric.get()</div><div class="line">        name2, loss2 = smoothl1_metric.get()</div><div class="line">    print(<span class="string">'epoch:'</span>, epoch, <span class="string">'class loss:'</span>, loss1, <span class="string">'bbox loss:'</span>, loss2)</div></pre></td></tr></table></figure>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总的来说，就这种复杂网络，官方基本都是基于4~8块顶级GPU进行训练，详情可见其官网<code>log</code>，更加复杂的甚至几十块，所以对于平民同学来说，更多的还是基于预训练的网络进行微调，在自己的小一些的数据集上做特定的应用，比如说定期检测乌龟跑到培养箱的哪个位置去了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;单发多框检测&quot;&gt;&lt;a href=&quot;#单发多框检测&quot; class=&quot;headerlink&quot; title=&quot;单发多框检测&quot;&gt;&lt;/a&gt;单发多框检测&lt;/h1&gt;&lt;p&gt;在目标检查的任务中，单发多框检测（single shot multibox detection，SSD）是一个计算量和性能都居中的算法，容易理解且实现不算复杂，广泛应用于很多检测模型中。&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Semantic Segmentation with FCN</title>
    <link href="http://shic.top/Deep-Learning/fcn/"/>
    <id>http://shic.top/Deep-Learning/fcn/</id>
    <published>2019-05-23T16:00:00.000Z</published>
    <updated>2019-05-24T08:56:01.675Z</updated>
    
    <content type="html"><![CDATA[<h1 id="全卷积网络FCN"><a href="#全卷积网络FCN" class="headerlink" title="全卷积网络FCN"></a>全卷积网络FCN</h1><h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h2><p>目标检测可以识别出物体在图像中的位置和种类，而语义分割（semantic segmentation）关注的是从像素级别的对图像分割成不同语义类别的区域，它比目标检测更加精细，可以定位到物体的边缘，其中实例分割（instance segmentation）还能区分同种类型的不同个体。<br><a id="more"></a><br>常见的用于语义分割的优秀模型例如：FCN，PSP，DeepLab等，实例分割例如Mark RCNN。</p>
<h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><p>全卷积网络（fully convolutional network，FCN）是用于语义分割的常见网络模型，它采用卷积神经网络实现了对图像像素级别的预测，但它又和传统的卷积神经网络（CNN）不同，普通CNN会把图像越变越小，且通道数越来越多。FCN通过将普通CNN最后的类别预测功能的层（通常是把特征张量变平的Flatten和输出用的全连接层）替换成转置卷积层（transposed convolution）。卷积计算可以通过矩阵乘法实现，卷积的前向计算可以看做是函数输入乘以权重矩阵𝑊，而反向传播依据链式法则，其梯度算得就是权重矩阵𝑊的转置（@有空改改，加上证明）。所以卷积层的前向计算函数与反向传播函数：这两个函数可以看作将函数输入向量分别乘以𝑊的转置和𝑊 。故通过转置卷积层可以实现类似反向传播的计算，也就是把输入的尺寸放大，把通道数变小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon, nd</div><div class="line">conv = gluon.nn.Conv2D(<span class="number">10</span>, kernel_size=<span class="number">4</span>, padding=<span class="number">1</span>, strides=<span class="number">2</span>)</div><div class="line">conv.initialize()</div><div class="line"></div><div class="line">X = nd.random.uniform(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>))</div><div class="line">Y = conv(X)</div><div class="line">Y.shape</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1</span>, <span class="number">10</span>, <span class="number">32</span>, <span class="number">32</span>)</div><div class="line"></div><div class="line">conv_trans = gluon.nn.Conv2DTranspose(<span class="number">3</span>, kernel_size=<span class="number">4</span>, padding=<span class="number">1</span>, strides=<span class="number">2</span>)</div><div class="line">conv_trans.initialize()</div><div class="line">conv_trans(Y).shape <span class="comment"># 同输入一样的形状，但是矩阵里面的值与x不同。</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)</div></pre></td></tr></table></figure>
<p><code>conv_trans(Y)</code>的形状与输入相同，但是矩阵里面的值与输入不同。用<code>numpy</code>看看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">x = np.arange(<span class="number">1</span>,<span class="number">5</span>).reshape([<span class="number">4</span>,<span class="number">1</span>])</div><div class="line">w = np.arange(<span class="number">1</span>,<span class="number">13</span>).reshape([<span class="number">3</span>,<span class="number">4</span>])</div><div class="line">y = np.dot(w,x)</div><div class="line">wt = np.transpose(w)</div><div class="line">backward_x = np.dot(wt,y)</div><div class="line">np.shape(backward_x) == np.shape(x)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">True</span></div></pre></td></tr></table></figure>
<h3 id="构造FCN"><a href="#构造FCN" class="headerlink" title="构造FCN"></a>构造FCN</h3><p>FCN由提取图像特征的普通卷积层和用于放大特征图的转置卷积层组成，我们可以采用基于ImageNet预训练的模型来提取图像特征，把后面用于预测的层替换成转置卷积层即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=<span class="keyword">True</span>)</div><div class="line">pretrained_net.features[<span class="number">-5</span>:] <span class="comment"># 打印features的最后五层</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>HybridSequential</div><div class="line">(</div><div class="line">  (<span class="number">0</span>): HybridSequential(</div><div class="line">    (<span class="number">0</span>): BasicBlockV2(</div><div class="line">      (bn1): BatchNorm(axis=<span class="number">1</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.9</span>, fix_gamma=<span class="keyword">False</span>, use_global_stats=<span class="keyword">False</span>, in_channels=<span class="number">256</span>)</div><div class="line">      (conv1): Conv2D(256 -&gt; 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</div><div class="line">      (bn2): BatchNorm(axis=<span class="number">1</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.9</span>, fix_gamma=<span class="keyword">False</span>, use_global_stats=<span class="keyword">False</span>, in_channels=<span class="number">512</span>)</div><div class="line">      (conv2): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</div><div class="line">      (downsample): Conv2D(256 -&gt; 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</div><div class="line">    )</div><div class="line">    (<span class="number">1</span>): BasicBlockV2(</div><div class="line">      (bn1): BatchNorm(axis=<span class="number">1</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.9</span>, fix_gamma=<span class="keyword">False</span>, use_global_stats=<span class="keyword">False</span>, in_channels=<span class="number">512</span>)</div><div class="line">      (conv1): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</div><div class="line">      (bn2): BatchNorm(axis=<span class="number">1</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.9</span>, fix_gamma=<span class="keyword">False</span>, use_global_stats=<span class="keyword">False</span>, in_channels=<span class="number">512</span>)</div><div class="line">      (conv2): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</div><div class="line">    )</div><div class="line">  )</div><div class="line">  (<span class="number">1</span>): BatchNorm(axis=<span class="number">1</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.9</span>, fix_gamma=<span class="keyword">False</span>, use_global_stats=<span class="keyword">False</span>, in_channels=<span class="number">512</span>)</div><div class="line">  (<span class="number">2</span>): Activation(relu)</div><div class="line">  (<span class="number">3</span>): GlobalAvgPool2D(size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">0</span>, <span class="number">0</span>), ceil_mode=<span class="keyword">True</span>)</div><div class="line">  (<span class="number">4</span>): Flatten</div><div class="line">)</div></pre></td></tr></table></figure>
<p>打印出最后五层可以看出（0~4），第(3),(4)是全局平均池化层和样本变平层，是把特征图计算得到的结果当做全连接层的预测做输入的，显然在FCN里需要把它们替换掉，仅保留提取特征的所有层。而最后的用于类别预测的全连接层也应该被替换掉.</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pretrained_net.output</div><div class="line">&gt;&gt;&gt; Dense(512 -&gt; 1000, linear)</div></pre></td></tr></table></figure>
<p>那么，FCN的结果就用过包含<code>pretrained_net</code>的<code>features</code>的除了最后两层的所有层，以及卷积置换层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">fcn = gluon.nn.HybridSequential()</div><div class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> pretrained_net.features[:<span class="number">-2</span>]: <span class="comment"># 从第一层到倒数第三层</span></div><div class="line">    fcn.add(layer)</div></pre></td></tr></table></figure>
<p>此时的<code>fcn</code>就是一个仅含有<code>resnet18_v2</code>的提取图像特征的网络，其输出也会是一个特征图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">X = nd.random.uniform(shape=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">320</span>, <span class="number">480</span>))</div><div class="line">fcn(X).shape</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1</span>, <span class="number">512</span>, <span class="number">10</span>, <span class="number">15</span>)</div></pre></td></tr></table></figure>
<p>可以见得，此时的特征图的长宽都已经被缩小了32倍了，而通道变厚到了512，我们首先可以通过1x1的卷积将通道数变为我们需要的数量。fcn中也用最后的每一个通道来预测每一个类，从而避免使用全连接层而降低模型复杂度（和ssd中的通道预测类别一样）。在接下来要使用的Pascal VOC2012数据集的类别个数为21，我们就可以设置一个通道数为21的1×1卷积和步幅为 𝑠 、填充为 𝑠/2 （假设 𝑠/2 为整数）、卷积核的高和宽为 2𝑠的置换卷积核，它正好能将输入放大s倍。这里的s根据前面的计算可以得出是32。所以置换卷积核的卷积核高和宽应该为64，步幅为32，填充为16。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">num_classes = <span class="number">21</span></div><div class="line"><span class="comment"># 往fcn中添加层得到一个完整的fcn</span></div><div class="line">fcn.add(nn.Conv2D(num_classes, kernel_size=<span class="number">1</span>), <span class="comment"># 改变通道数的1*1卷积</span></div><div class="line">        nn.Conv2DTranspose(num_classes, kernel_size=<span class="number">64</span>, padding=<span class="number">16</span>, strides=<span class="number">32</span>)) <span class="comment"># 放大输入</span></div></pre></td></tr></table></figure>
<p>现在fcn中第一层到倒数第三层都是基于ImageNet预训练的ResNet18_v2的对应层的参数，而后面的(1,1,classes_num)卷积层和置换卷积层的参数还没有，我们首先需要初始化。在置换卷积层中，需要放大特征图，必定会导致输出的像素点多于输入，也就是输出的矩阵比输入更大，这就是上采样（upsample）。双线性插值是上采样的一种方法。（@有空可解释一下双线性插值）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 双线性插值</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bilinear_kernel</span><span class="params">(in_channels, out_channels, kernel_size)</span>:</span></div><div class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span></div><div class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</div><div class="line">        center = factor - <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        center = factor - <span class="number">0.5</span></div><div class="line">    og = np.ogrid[:kernel_size, :kernel_size]</div><div class="line">    filt = (<span class="number">1</span> - abs(og[<span class="number">0</span>] - center) / factor) * \</div><div class="line">           (<span class="number">1</span> - abs(og[<span class="number">1</span>] - center) / factor)</div><div class="line">    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),</div><div class="line">                      dtype=<span class="string">'float32'</span>)</div><div class="line">    weight[range(in_channels), range(out_channels), :, :] = filt</div><div class="line">    <span class="keyword">return</span> nd.array(weight)</div><div class="line"></div><div class="line"><span class="comment"># 初始化最后两层</span></div><div class="line">fcn[<span class="number">-2</span>].initialize(mx.init=init.Xavier()) <span class="comment"># 1*1卷积就用普通初始化</span></div><div class="line">fcn[<span class="number">-1</span>].initialize(mx.init.Constant(bilinear_kernel(num_classes, num_classes, <span class="number">64</span>)))</div></pre></td></tr></table></figure>
<h2 id="Pascal-VOC2012数据集"><a href="#Pascal-VOC2012数据集" class="headerlink" title="Pascal VOC2012数据集"></a>Pascal VOC2012数据集</h2><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h3><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank" rel="external">Pascal VOC2012数据集</a>是一个用于目标检测和语义分割的著名数据集，是VOC2007的续集，VOC数据集系列的文件结构如下：</p>
<ul>
<li>VOCdevkit<ul>
<li>VOC2007<ul>
<li>…</li>
</ul>
</li>
<li>VOC2012<ul>
<li>Annotations：存放目标检测的标签</li>
<li>ImageSets：存放各种用途文件的名字</li>
<li>JPEGImages：图片文件</li>
<li>SegmentationClass：语义分割的标注</li>
<li>SegmentationObject：实例分割的标注</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>在官网下载好数据集解压后，我们可以按照自己的任务需求来使用数据集。我们的工作主要是要搭建起fcn，对于数据读取就简单略过，mxnet提供了一个很好用的工具包<a href="https://github.com/d2l-ai/d2l-zh/blob/master/d2lzh/utils.py" target="_blank" rel="external">d2lzh</a>，我们可以执行<code>pip install d2lzh</code>来安装它。之后便可以用它来读取数据了。其实gluoncv工具包也提供了读取该数据集的函数，也能很方便的使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line"></div><div class="line">crop_size  = (<span class="number">320</span>, <span class="number">480</span>) <span class="comment"># 随机裁剪成该大小，方便最批数据</span></div><div class="line">batch_size = <span class="number">32</span></div><div class="line">colormap2label = nd.zeros(<span class="number">256</span>**<span class="number">3</span>) <span class="comment"># 存放类别颜色到标签的映射关系的矩阵</span></div><div class="line"><span class="keyword">for</span> i, cm <span class="keyword">in</span> enumerate(d2l.VOC_COLORMAP):</div><div class="line">    colormap2label[(cm[<span class="number">0</span>] * <span class="number">256</span> + cm[<span class="number">1</span>]) * <span class="number">256</span> + cm[<span class="number">2</span>]] = i</div><div class="line">voc_dir = <span class="string">'../VOCdevkit/VOC2012'</span> <span class="comment"># 类似于这样的格式，最后是VOC2012</span></div><div class="line"></div><div class="line">num_workers = <span class="number">0</span> <span class="keyword">if</span> sys.platform.startswith(<span class="string">'win32'</span>) <span class="keyword">else</span> <span class="number">4</span> <span class="comment"># 64位电脑就多线程运行</span></div><div class="line"><span class="comment"># 1114个训练样本</span></div><div class="line">train_iter = gluon.data.DataLoader(</div><div class="line">    d2l.VOCSegDataset(<span class="keyword">True</span>, crop_size, voc_dir, colormap2label), batch_size,</div><div class="line">    shuffle=<span class="keyword">True</span>, last_batch=<span class="string">'discard'</span>, num_workers=num_workers)</div><div class="line"><span class="comment"># 1078个测试样本</span></div><div class="line">test_iter = gluon.data.DataLoader(</div><div class="line">    d2l.VOCSegDataset(<span class="keyword">False</span>, crop_size, voc_dir, colormap2label), batch_size,</div><div class="line">    last_batch=<span class="string">'discard'</span>, num_workers=num_workers)</div></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    ctx = [mx.gpu()]</div><div class="line">    _ = nd.array([<span class="number">0</span>], ctx=ctx)</div><div class="line"><span class="keyword">except</span> mx.base.MXNetError:</div><div class="line">        ctx = [mx.cpu()]</div><div class="line"></div><div class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=<span class="number">1</span>)</div><div class="line">fcn.collect_params().reset_ctx(ctx)</div><div class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'wd'</span>: <span class="number">1e-3</span>&#125;)</div><div class="line">d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs=<span class="number">12</span>)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>training on [gpu(<span class="number">0</span>)]</div><div class="line">epoch <span class="number">1</span>, loss <span class="number">1.1867</span>, train acc <span class="number">0.750</span>, test acc <span class="number">0.823</span>, time <span class="number">55.4</span> sec</div><div class="line">epoch <span class="number">2</span>, loss <span class="number">0.5028</span>, train acc <span class="number">0.846</span>, test acc <span class="number">0.823</span>, time <span class="number">54.8</span> sec</div><div class="line">epoch <span class="number">3</span>, loss <span class="number">0.4053</span>, train acc <span class="number">0.867</span>, test acc <span class="number">0.845</span>, time <span class="number">54.2</span> sec</div><div class="line">epoch <span class="number">4</span>, loss <span class="number">0.3393</span>, train acc <span class="number">0.888</span>, test acc <span class="number">0.850</span>, time <span class="number">54.1</span> sec</div><div class="line">epoch <span class="number">5</span>, loss <span class="number">0.2892</span>, train acc <span class="number">0.902</span>, test acc <span class="number">0.848</span>, time <span class="number">55.0</span> sec</div><div class="line">epoch <span class="number">6</span>, loss <span class="number">0.2496</span>, train acc <span class="number">0.915</span>, test acc <span class="number">0.857</span>, time <span class="number">53.7</span> sec</div><div class="line">epoch <span class="number">7</span>, loss <span class="number">0.2340</span>, train acc <span class="number">0.919</span>, test acc <span class="number">0.857</span>, time <span class="number">54.4</span> sec</div><div class="line">epoch <span class="number">8</span>, loss <span class="number">0.2244</span>, train acc <span class="number">0.923</span>, test acc <span class="number">0.860</span>, time <span class="number">53.4</span> sec</div><div class="line">epoch <span class="number">9</span>, loss <span class="number">0.2125</span>, train acc <span class="number">0.926</span>, test acc <span class="number">0.860</span>, time <span class="number">53.7</span> sec</div><div class="line">epoch <span class="number">10</span>, loss <span class="number">0.1951</span>, train acc <span class="number">0.931</span>, test acc <span class="number">0.860</span>, time <span class="number">53.6</span> sec</div><div class="line">epoch <span class="number">11</span>, loss <span class="number">0.1845</span>, train acc <span class="number">0.935</span>, test acc <span class="number">0.863</span>, time <span class="number">54.5</span> sec</div><div class="line">epoch <span class="number">12</span>, loss <span class="number">0.1761</span>, train acc <span class="number">0.938</span>, test acc <span class="number">0.862</span>, time <span class="number">53.7</span> sec</div></pre></td></tr></table></figure>
<p>训练中还是有值得去做的工作的，因为只有一块小GPU，提取特征的基础网络选得很小，训练也没训练多久，在后期精确度提升很小的情况下应该进一步降低学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(img)</span>:</span></div><div class="line">    X = test_iter._dataset.normalize_image(img) <span class="comment"># 标准化</span></div><div class="line">    X = X.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).expand_dims(axis=<span class="number">0</span>) <span class="comment">#转换成nn需要的4维输入格式</span></div><div class="line">    pred = nd.argmax(net(X.as_in_context(ctx[<span class="number">0</span>])), axis=<span class="number">1</span>)</div><div class="line">    <span class="keyword">return</span> pred.reshape((pred.shape[<span class="number">1</span>], pred.shape[<span class="number">2</span>]))</div><div class="line">  </div><div class="line"><span class="comment"># 可视化每个像素的预测类别，把标签值映射回颜色</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">label2image</span><span class="params">(pred)</span>:</span></div><div class="line">    colormap = nd.array(d2l.VOC_COLORMAP, ctx=ctx[<span class="number">0</span>], dtype=<span class="string">'uint8'</span>)</div><div class="line">    X = pred.astype(<span class="string">'int32'</span>)</div><div class="line">    <span class="keyword">return</span> colormap[X, :]</div><div class="line">  </div><div class="line"><span class="comment"># 读取测试数据和标签</span></div><div class="line">test_images, test_labels = d2l.read_voc_images(is_train=<span class="keyword">False</span>)</div><div class="line">n, imgs = <span class="number">4</span>, []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">    crop_rect = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">480</span>, <span class="number">320</span>) <span class="comment"># 裁剪</span></div><div class="line">    X = mx.image.fixed_crop(test_images[i], *crop_rect)</div><div class="line">    pred = label2image(predict(X))</div><div class="line">    imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)]</div><div class="line">d2l.show_images(imgs[::<span class="number">3</span>] + imgs[<span class="number">1</span>::<span class="number">3</span>] + imgs[<span class="number">2</span>::<span class="number">3</span>], <span class="number">3</span>, n);</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
<p><img src="https://github.com/Shicc/webrecourse/raw/master/img/fcn_pred.png" alt="fcn_pred"><br>此时，从<code>ResNet</code>继承了特征提取层的fcn网络的最后几层如下所示，第(11),(12)就是自己改的1*1卷积和转置卷积层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  (<span class="number">9</span>): BatchNorm(axis=<span class="number">1</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.9</span>, fix_gamma=<span class="keyword">False</span>, use_global_stats=<span class="keyword">False</span>, in_channels=<span class="number">512</span>)</div><div class="line">  (<span class="number">10</span>): Activation(relu)</div><div class="line">  (11): Conv2D(512 -&gt; 21, kernel_size=(1, 1), stride=(1, 1))</div><div class="line">  (12): Conv2DTranspose(21 -&gt; 21, kernel_size=(64, 64), stride=(32, 32), padding=(16, 16))</div><div class="line">)</div></pre></td></tr></table></figure>
<h2 id="小结以及model-zoo的正确使用方法"><a href="#小结以及model-zoo的正确使用方法" class="headerlink" title="小结以及model_zoo的正确使用方法"></a>小结以及<code>model_zoo</code>的正确使用方法</h2><p>总的来说预测结果一般吧，如果有用到fcn来做自己的语义分割任务，还是比较建议微调<code>model_zoo</code>里已有的模型，通过重新设置自己需要的类别和需要继承的权重，然后根据自己的任务要求，用适合任务的数据集微调这个模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 重新设置类别和重用权重</span></div><div class="line">net.reset_class(classes=[<span class="string">'person'</span>], reuse_weights=[<span class="string">'person'</span>])</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;全卷积网络FCN&quot;&gt;&lt;a href=&quot;#全卷积网络FCN&quot; class=&quot;headerlink&quot; title=&quot;全卷积网络FCN&quot;&gt;&lt;/a&gt;全卷积网络FCN&lt;/h1&gt;&lt;h2 id=&quot;语义分割&quot;&gt;&lt;a href=&quot;#语义分割&quot; class=&quot;headerlink&quot; title=&quot;语义分割&quot;&gt;&lt;/a&gt;语义分割&lt;/h2&gt;&lt;p&gt;目标检测可以识别出物体在图像中的位置和种类，而语义分割（semantic segmentation）关注的是从像素级别的对图像分割成不同语义类别的区域，它比目标检测更加精细，可以定位到物体的边缘，其中实例分割（instance segmentation）还能区分同种类型的不同个体。&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Finetuning-classification</title>
    <link href="http://shic.top/Deep-Learning/finetune-classification/"/>
    <id>http://shic.top/Deep-Learning/finetune-classification/</id>
    <published>2019-05-10T16:00:00.000Z</published>
    <updated>2019-05-11T12:54:23.698Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从头训练or微调模型：迁移学习快速实战"><a href="#从头训练or微调模型：迁移学习快速实战" class="headerlink" title="从头训练or微调模型：迁移学习快速实战"></a>从头训练or微调模型：迁移学习快速实战</h1><h2 id="为什么要做？"><a href="#为什么要做？" class="headerlink" title="为什么要做？"></a>为什么要做？</h2><p>还记得2018年微软创新杯上，我和我同学牛牛的项目中需要训练一个智能识别食物种类的模型。当时没有计算资源的我们，写完代码用CPU跑了跑就提交了，止步复赛。今年一大段无法活动的时间里，整理了以前的代码，我捉摸着观察这一神奇的数据集，看着未训练的模型，心想着这东西必须得整一下，也为之后的某次盘问收集点会议资料，待到半年后大伤初愈后，估计又得上路了。<br><a id="more"></a></p>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>在计算机视觉上，如果从头训练一个模型，我们不经需要足量的数据集来满足训练要求，也需要足够多的计算资源，显然这不是每一个人都能达到的，有一种办法那就是迁移学习（transfer learning），它将从源数据集学到的知识迁移到目标数据集上，预训练模型可以抽取较为通用的图像特征，只需更改模型的输出层并重新训练，使之满足目标数据集的要求。微调（fine tuning）是迁移学习中的一种常用技术。微调的意思就是对于目的模型的features采用预训练模型的features并用很小的学习率微调或者就固定不变，而输出层采用随机初始化并从头开始训练。</p>
<h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>在kaggle上，合适的数据集就是<a href="https://www.kaggle.com/prathmeshgodse/food101-zip" target="_blank" rel="external">food101</a>了。但神奇的是9GB大的数据集中，训练集和测试集内容都是一样的。严格意义上讲，测试集只能在模型的各种参数确定后用来测试模型的精度，但训练集和测试集一样的话，那相当于还是用的训练集在做测试，这样得出的泛化误差那就是训练误差，是不对的。当时做的时候也没管，用CPU去跑这个庞大的数据集也没能体验到fine-tuning的魅力，对于最后一个101类的分类器也不好训练，其实fing-tuning没啥东西，但有时候懂算法就是那一个全新的任务不好上手，跑不好模型还是时有发生的，这次闲着无聊整了一个5个类的小数据集，且正确划分了训练集和测试集。冥冥之中觉得又一个上手fing-tuning的好例子，小型数据集：<a href="https://pan.baidu.com/s/1Oaf8m5IZiwLOBqc9gJ4aDw" target="_blank" rel="external">网盘</a> 提取码: 8u46</p>
<p>训练集各类各700张，测试集各类各300张，其文件结构如下：</p>
<ul>
<li>food5<ul>
<li>train<ul>
<li>apple_pie</li>
<li>baby_back_ribs</li>
<li>baklava</li>
<li>beef_carpaccio</li>
<li>beef_tartare</li>
</ul>
</li>
<li>test<ul>
<li>apple_pie</li>
<li>baby_back_ribs</li>
<li>baklava</li>
<li>beef_carpaccio</li>
<li>beef_tartare</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</div><div class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon, init</div><div class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata, loss <span class="keyword">as</span> gloss, model_zoo</div><div class="line"><span class="keyword">import</span> os</div></pre></td></tr></table></figure>
<h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p>gluon提供了一个读取用文件夹名字分好类的图像数据集接口，同一类别的数据都放在同一文件夹中，文件夹名就是类别名。函数会自动生成相应的label。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_imgs = gdata.vision.ImageFolderDataset(os.path.join(data_dir, <span class="string">'train'</span>))</div><div class="line">test_imgs = gdata.vision.ImageFolderDataset(os.path.join(data_dir, <span class="string">'test'</span>))</div></pre></td></tr></table></figure>
<p>得到的train_imgs和test_imgs就是一个<code>ImageFolderDataset</code>类，通过<code>__len__()</code>可以得到其长度，即包含多少个样本， <code>.__getitem__(x)</code>会返回第x个样本和其类别。</p>
<p>处理数据时，我们对RGB三个颜色通道的数值做标准化，每个数值减去该通道所有数值的平均值，再除以该通道所有数值的标准差作为输出。当图片大小不一样时，我们先从图像中裁剪出随机大小和随机高宽比的一块随机区域，然后将该区域缩放为高和宽均为224像素的输入。测试时，我们将图像的高和宽均缩放为256像素，然后从中裁剪出高和宽均为224像素的中心区域作为输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 指定RGB三个通道的均值和方差来将图像通道归一化</span></div><div class="line"><span class="comment"># 此数据是ImageNet上的数据算出来的数值</span></div><div class="line">normalize = gdata.vision.transforms.Normalize(</div><div class="line">    [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</div><div class="line"></div><div class="line">train_augs = gdata.vision.transforms.Compose([</div><div class="line">    gdata.vision.transforms.RandomResizedCrop(<span class="number">224</span>),</div><div class="line">    gdata.vision.transforms.RandomFlipLeftRight(),</div><div class="line">    gdata.vision.transforms.ToTensor(),</div><div class="line">    normalize])</div><div class="line"></div><div class="line">test_augs = gdata.vision.transforms.Compose([</div><div class="line">    gdata.vision.transforms.Resize(<span class="number">256</span>),</div><div class="line">    gdata.vision.transforms.CenterCrop(<span class="number">224</span>),</div><div class="line">    gdata.vision.transforms.ToTensor(),</div><div class="line">    normalize])</div></pre></td></tr></table></figure>
<p>调用<code>ImageFolderDataset</code>类的<code>transform_first(train_augs)</code>函数，传入变化的规则，它会把数据集的全部输入按照规则来变化而保留label不动，这就得到了我们数据处理后的可以拿来训练的数据集。在通过gluon提供的<code>DataLoader</code>类来获取批量数据，<code>DataLoader</code>类接受三个参数，第一个是数据集，第二个是<code>batch_size</code>，会把数据集按照<code>batch_size</code>来划分，第三个指定是否随机读取的，<code>shuffle=True</code>表示随机读取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_iter = gdata.DataLoader(train_imgs.transform_first(train_augs), batch_size, shuffle=<span class="keyword">True</span>)</div><div class="line">test_iter = gdata.DataLoader(test_imgs.transform_first(test_augs), batch_size)</div></pre></td></tr></table></figure>
<h2 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h2><p>以前使用的<code>NumPy</code>的<code>vgg</code>参数文件基于<code>TensorFlow</code>1.x的代码做fine tuning很麻烦，代码臃肿还不好理解，详情请看这个<a href="https://github.com/Shicc/food101" target="_blank" rel="external">repo</a>。<code>mxnet</code>的<code>Gluon</code>前端出来后，便利上也广受好评，本次就放弃原先的代码，用<code>Gluon</code>3行代码快速做好fine tuning。</p>
<h3 id="定义微调模型"><a href="#定义微调模型" class="headerlink" title="定义微调模型"></a>定义微调模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 采用在ImageNet上预训练的resnet34_v2网络</span></div><div class="line">pretrained_net = model_zoo.vision.resnet34_v2(pretrained=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">finetune_net = model_zoo.vision.resnet34_v2(classes=<span class="number">5</span>) <span class="comment">#classes表示类别数</span></div><div class="line"><span class="comment"># 固定features中的参数：</span></div><div class="line"><span class="comment"># finetune_net.features.collect_params().setattr('grad_req', 'null')</span></div><div class="line">finetune_net.features = pretrained_net.features</div><div class="line">finetune_net.output.initialize(init.Xavier())</div><div class="line"><span class="comment"># output中的模型参数将在迭代中使用10倍大的学习率</span></div><div class="line">finetune_net.output.collect_params().setattr(<span class="string">'lr_mult'</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<h3 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fine_tuning</span><span class="params">(net, learning_rate, batch_size=<span class="number">200</span>, num_epochs=<span class="number">20</span>)</span>:</span></div><div class="line">    ctx = d2l.try_all_gpus()</div><div class="line">    net.collect_params().reset_ctx(ctx) <span class="comment"># 把数据复制到gpu上</span></div><div class="line">    net.hybridize()</div><div class="line">    loss = gloss.SoftmaxCrossEntropyLoss()</div><div class="line">    trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;</div><div class="line">        <span class="string">'learning_rate'</span>: learning_rate, <span class="string">'wd'</span>: <span class="number">0.001</span>&#125;)</div><div class="line">    d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)</div><div class="line"></div><div class="line">train_fine_tuning(finetune_net, <span class="number">0.01</span>) <span class="comment">#用0.01的小学习率微调features</span></div></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>简单跑了一下结果也还能接受，但从最后几个epochs看来，还有优化空间</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">training on [gpu(0)]</div><div class="line">epoch 1, loss 3.8275, train acc 0.401, test acc 0.652, time 1299.6 sec</div><div class="line">epoch 2, loss 1.1384, train acc 0.680, test acc 0.772, time 50.4 sec</div><div class="line">epoch 3, loss 0.5352, train acc 0.817, test acc 0.801, time 51.5 sec</div><div class="line">epoch 4, loss 0.4826, train acc 0.828, test acc 0.842, time 50.4 sec</div><div class="line">epoch 5, loss 0.4427, train acc 0.842, test acc 0.813, time 49.8 sec</div><div class="line">epoch 6, loss 0.3969, train acc 0.858, test acc 0.856, time 50.5 sec</div><div class="line">epoch 7, loss 0.3310, train acc 0.879, test acc 0.867, time 49.6 sec</div><div class="line">epoch 8, loss 0.3035, train acc 0.893, test acc 0.818, time 50.7 sec</div><div class="line">epoch 9, loss 0.2848, train acc 0.895, test acc 0.865, time 51.3 sec</div><div class="line">epoch 10, loss 0.3183, train acc 0.883, test acc 0.865, time 49.4 sec</div><div class="line">epoch 11, loss 0.2798, train acc 0.895, test acc 0.875, time 50.3 sec</div><div class="line">epoch 12, loss 0.2640, train acc 0.902, test acc 0.861, time 51.0 sec</div><div class="line">epoch 13, loss 0.2406, train acc 0.915, test acc 0.877, time 49.7 sec</div><div class="line">epoch 14, loss 0.2262, train acc 0.918, test acc 0.875, time 50.3 sec</div><div class="line">epoch 15, loss 0.1985, train acc 0.929, test acc 0.869, time 50.4 sec</div><div class="line">epoch 16, loss 0.2123, train acc 0.923, test acc 0.883, time 50.4 sec</div><div class="line">epoch 17, loss 0.1978, train acc 0.925, test acc 0.886, time 50.3 sec</div><div class="line">epoch 18, loss 0.1935, train acc 0.929, test acc 0.884, time 49.9 sec</div><div class="line">epoch 19, loss 0.1899, train acc 0.934, test acc 0.878, time 50.4 sec</div><div class="line">epoch 20, loss 0.1936, train acc 0.931, test acc 0.894, time 50.5 sec</div></pre></td></tr></table></figure>
<h3 id="gpu使用情况"><a href="#gpu使用情况" class="headerlink" title="gpu使用情况"></a>gpu使用情况</h3><p>只是没想到这么少的数据都占了这么多显存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |</div><div class="line">| N/A   77C    P0    31W /  70W |  15071MiB / 15079MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID   Type   Process name                             Usage      |</div><div class="line">|=============================================================================|</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure>
<p>而finetuning_net的输出层则被改成了5个类</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;pretrained_net.output</div><div class="line">Dense(2048 -&gt; 5, linear)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;从头训练or微调模型：迁移学习快速实战&quot;&gt;&lt;a href=&quot;#从头训练or微调模型：迁移学习快速实战&quot; class=&quot;headerlink&quot; title=&quot;从头训练or微调模型：迁移学习快速实战&quot;&gt;&lt;/a&gt;从头训练or微调模型：迁移学习快速实战&lt;/h1&gt;&lt;h2 id=&quot;为什么要做？&quot;&gt;&lt;a href=&quot;#为什么要做？&quot; class=&quot;headerlink&quot; title=&quot;为什么要做？&quot;&gt;&lt;/a&gt;为什么要做？&lt;/h2&gt;&lt;p&gt;还记得2018年微软创新杯上，我和我同学牛牛的项目中需要训练一个智能识别食物种类的模型。当时没有计算资源的我们，写完代码用CPU跑了跑就提交了，止步复赛。今年一大段无法活动的时间里，整理了以前的代码，我捉摸着观察这一神奇的数据集，看着未训练的模型，心想着这东西必须得整一下，也为之后的某次盘问收集点会议资料，待到半年后大伤初愈后，估计又得上路了。&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot+Spring Data JPA+Thymeleaf(一)</title>
    <link href="http://shic.top/Spring-Boot/Spring%20Boot1/"/>
    <id>http://shic.top/Spring-Boot/Spring Boot1/</id>
    <published>2018-03-07T16:00:10.000Z</published>
    <updated>2018-03-08T05:52:47.094Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Spring Boot：颠覆Java EE开发</strong><br><a id="more"></a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>或许你听过Spring,Struts2,Hibernate的SSH整合开发框架，你可能也用过Spring,SpringMVC,Mybatis的SSM,但它们都或多或少的有些麻烦，大量的配置文件也不利于专注于代码的编写。那么全新的Spring Boot+Spring Data JPA+Thymeleaf开发模式能让你感觉原来Java Web也可以变得简单<br><br><br>Spring Boot在2014年面市后，一路高歌猛进，大有颠覆Java EE开发的趋势，它的配置少，对其他框架有整合开发的特点，能让编码人员专注于自己的业务代码的编写而不会时不时就是编写大量的配置文件。让开发也变得高效和便捷。如果你也想学习Java Web的开发，那么直接学习Spring Boot是你最好的选择！<br><br></p>
<h2 id="关于本套视频"><a href="#关于本套视频" class="headerlink" title="关于本套视频"></a>关于本套视频</h2><p>视频制作于2017年11月初，本次对视频资源做了以下调整和优化：</p>
<ul>
<li>视频以一个用户登录注册的例子来进行讲解，不会涉及太多功能和代码，主要起普及Spring Boot的作用，方便初学者学习和入门Spring Boot。</li>
<li>切分视频为6段，分段学习降低因篇幅过长的困扰</li>
<li>去除广告的困扰，看视频没广告</li>
<li>视频清晰度全调整为1080P的全高清，代码段清晰可见</li>
<li>由于没有写过稿子直接讲解和录制的，所以难免在会有点语误，还希望大家理解。如有不理解的地方，请在某一节的视频页面下面留言。</li>
</ul>
<h2 id="上车学习"><a href="#上车学习" class="headerlink" title="上车学习"></a>上车学习</h2><p><strong>第一节：简单介绍了一下spring boot，从打开IED开始，到配置yml文件止</strong></p>
<iframe height="366" width="650" src="https://www.yylep.com/f-1609-ck/36a0445a.flv?m3u8=1" controls="controls">allowfullscreen=”allowfullscreen”&gt;</iframe>

]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Spring Boot：颠覆Java EE开发&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spring Boot" scheme="http://shic.top/categories/Spring-Boot/"/>
    
    
      <category term="Spring Boot" scheme="http://shic.top/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot+Spring Data JPA+Thymeleaf(二)</title>
    <link href="http://shic.top/Spring-Boot/Spring%20Boot2/"/>
    <id>http://shic.top/Spring-Boot/Spring Boot2/</id>
    <published>2018-03-07T16:00:09.000Z</published>
    <updated>2018-03-08T05:30:26.716Z</updated>
    
    <content type="html"><![CDATA[<p><strong>第二节：配置yml文件-&gt;在数据库查找学生</strong><br><a id="more"></a></p>
<h2 id="关于本套视频"><a href="#关于本套视频" class="headerlink" title="关于本套视频"></a>关于本套视频</h2><p>视频制作于2017年11月初，本次对视频资源做了以下调整和优化：</p>
<ul>
<li>视频以一个用户登录注册的例子来进行讲解，不会涉及太多功能和代码，主要起普及Spring Boot的作用，方便初学者学习和入门Spring Boot。</li>
<li>切分视频为6段，分段学习降低因篇幅过长的困扰</li>
<li>去除广告的困扰，看视频没广告</li>
<li>视频清晰度全调整为1080P的全高清，代码段清晰可见</li>
<li>由于没有写过稿子直接讲解和录制的，所以难免在会有点语误，还希望大家理解。如有不理解的地方，请在某一节的视频页面下面留言。</li>
</ul>
<h2 id="上车学习"><a href="#上车学习" class="headerlink" title="上车学习"></a>上车学习</h2><iframe height="366" width="650" src="https://www.yylep.com/f-1609-ck/bc5a8ed7.flv?m3u8=1" frameborder="0" allowfullscreen="allowfullscreen"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;第二节：配置yml文件-&amp;gt;在数据库查找学生&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spring Boot" scheme="http://shic.top/categories/Spring-Boot/"/>
    
    
      <category term="Spring Boot" scheme="http://shic.top/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot+Spring Data JPA+Thymeleaf(三)</title>
    <link href="http://shic.top/Spring-Boot/Spring%20Boot3/"/>
    <id>http://shic.top/Spring-Boot/Spring Boot3/</id>
    <published>2018-03-07T16:00:08.000Z</published>
    <updated>2018-03-08T05:30:22.205Z</updated>
    
    <content type="html"><![CDATA[<p><strong>第三节：讲解数据库连接层的测试</strong><br><a id="more"></a></p>
<h2 id="关于本套视频"><a href="#关于本套视频" class="headerlink" title="关于本套视频"></a>关于本套视频</h2><p>视频制作于2017年11月初，本次对视频资源做了以下调整和优化：</p>
<ul>
<li>视频以一个用户登录注册的例子来进行讲解，不会涉及太多功能和代码，主要起普及Spring Boot的作用，方便初学者学习和入门Spring Boot。</li>
<li>切分视频为6段，分段学习降低因篇幅过长的困扰</li>
<li>去除广告的困扰，看视频没广告</li>
<li>视频清晰度全调整为1080P的全高清，代码段清晰可见</li>
<li>由于没有写过稿子直接讲解和录制的，所以难免在会有点语误，还希望大家理解。如有不理解的地方，请在某一节的视频页面下面留言。</li>
</ul>
<h2 id="上车学习"><a href="#上车学习" class="headerlink" title="上车学习"></a>上车学习</h2><iframe height="366" width="650" src="https://www.yylep.com/f-1609-ck/dbfa1e57.flv?m3u8=1" frameborder="0" allowfullscreen="allowfullscreen"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;第三节：讲解数据库连接层的测试&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spring Boot" scheme="http://shic.top/categories/Spring-Boot/"/>
    
    
      <category term="Spring Boot" scheme="http://shic.top/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot+Spring Data JPA+Thymeleaf(四)</title>
    <link href="http://shic.top/Spring-Boot/Spring%20Boot4/"/>
    <id>http://shic.top/Spring-Boot/Spring Boot4/</id>
    <published>2018-03-07T16:00:07.000Z</published>
    <updated>2018-03-08T05:30:17.452Z</updated>
    
    <content type="html"><![CDATA[<p><strong>第四节：讲解控制层的编写</strong><br><a id="more"></a></p>
<h2 id="关于本套视频"><a href="#关于本套视频" class="headerlink" title="关于本套视频"></a>关于本套视频</h2><p>视频制作于2017年11月初，本次对视频资源做了以下调整和优化：</p>
<ul>
<li>视频以一个用户登录注册的例子来进行讲解，不会涉及太多功能和代码，主要起普及Spring Boot的作用，方便初学者学习和入门Spring Boot。</li>
<li>切分视频为6段，分段学习降低因篇幅过长的困扰</li>
<li>去除广告的困扰，看视频没广告</li>
<li>视频清晰度全调整为1080P的全高清，代码段清晰可见</li>
<li>由于没有写过稿子直接讲解和录制的，所以难免在会有点语误，还希望大家理解。如有不理解的地方，请在某一节的视频页面下面留言。</li>
</ul>
<h2 id="上车学习"><a href="#上车学习" class="headerlink" title="上车学习"></a>上车学习</h2><iframe height="366" width="650" src="https://www.yylep.com/f-1609-ck/37bdaed9.flv?m3u8=1" frameborder="0" allowfullscreen="allowfullscreen"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;第四节：讲解控制层的编写&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spring Boot" scheme="http://shic.top/categories/Spring-Boot/"/>
    
    
      <category term="Spring Boot" scheme="http://shic.top/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot+Spring Data JPA+Thymeleaf(五）</title>
    <link href="http://shic.top/Spring-Boot/Spring%20Boot5/"/>
    <id>http://shic.top/Spring-Boot/Spring Boot5/</id>
    <published>2018-03-07T16:00:06.000Z</published>
    <updated>2018-03-08T06:00:37.445Z</updated>
    
    <content type="html"><![CDATA[<p><strong>第五节：完善控制层，结合Thymeleaf做前端</strong><br><a id="more"></a></p>
<h2 id="关于本套视频"><a href="#关于本套视频" class="headerlink" title="关于本套视频"></a>关于本套视频</h2><p>视频制作于2017年11月初，本次对视频资源做了以下调整和优化：</p>
<ul>
<li>视频以一个用户登录注册的例子来进行讲解，不会涉及太多功能和代码，主要起普及Spring Boot的作用，方便初学者学习和入门Spring Boot。</li>
<li>切分视频为6段，分段学习降低因篇幅过长的困扰</li>
<li>去除广告的困扰，看视频没广告</li>
<li>视频清晰度全调整为1080P的全高清，代码段清晰可见</li>
<li>由于没有写过稿子直接讲解和录制的，所以难免在会有点语误，还希望大家理解。如有不理解的地方，请在某一节的视频页面下面留言。</li>
</ul>
<h2 id="上车学习"><a href="#上车学习" class="headerlink" title="上车学习"></a>上车学习</h2><iframe height="366" width="650" src="https://www.yylep.com/f-1609-ck/501d3e59.flv?m3u8=1" frameborder="0" allowfullscreen="allowfullscreen"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;第五节：完善控制层，结合Thymeleaf做前端&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spring Boot" scheme="http://shic.top/categories/Spring-Boot/"/>
    
    
      <category term="Spring Boot" scheme="http://shic.top/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot+Spring Data JPA+Thymeleaf(六)</title>
    <link href="http://shic.top/Spring-Boot/Spring%20Boot6/"/>
    <id>http://shic.top/Spring-Boot/Spring Boot6/</id>
    <published>2018-03-07T16:00:05.000Z</published>
    <updated>2018-03-08T06:33:18.551Z</updated>
    
    <content type="html"><![CDATA[<p><strong>第六节：测试和结尾</strong><br><a id="more"></a></p>
<h2 id="关于本套视频"><a href="#关于本套视频" class="headerlink" title="关于本套视频"></a>关于本套视频</h2><p>视频制作于2017年11月初，本次对视频资源做了以下调整和优化：</p>
<ul>
<li>视频以一个用户登录注册的例子来进行讲解，不会涉及太多功能和代码，主要起普及Spring Boot的作用，方便初学者学习和入门Spring Boot。</li>
<li>切分视频为6段，分段学习降低因篇幅过长的困扰</li>
<li>去除广告的困扰，看视频没广告</li>
<li>视频清晰度全调整为1080P的全高清，代码段清晰可见</li>
<li>由于没有写过稿子直接讲解和录制的，所以难免在会有点语误，还希望大家理解。如有不理解的地方，请在某一节的视频页面下面留言。</li>
</ul>
<h2 id="上车学习"><a href="#上车学习" class="headerlink" title="上车学习"></a>上车学习</h2><iframe height="366" width="650" src="https://www.yylep.com/p-1609-ck?path=/计算机学习/spring boot自制/6C.flv" frameborder="0" allowfullscreen="allowfullscreen"></iframe>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;第六节：测试和结尾&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Spring Boot" scheme="http://shic.top/categories/Spring-Boot/"/>
    
    
      <category term="Spring Boot" scheme="http://shic.top/tags/Spring-Boot/"/>
    
  </entry>
  
  <entry>
    <title>爬爬音乐</title>
    <link href="http://shic.top/Python/%E7%88%AC%E7%88%AC%E9%9F%B3%E4%B9%90/"/>
    <id>http://shic.top/Python/爬爬音乐/</id>
    <published>2018-02-20T16:00:01.000Z</published>
    <updated>2018-02-21T09:55:05.393Z</updated>
    
    <content type="html"><![CDATA[<p>喜欢音乐请支持正版，代码仅作学习交流。<br><a id="more"></a><br>意外发现国内某知名音乐网站很好爬，也没有做什么反爬机制。电脑太卡已经容不下一个任何一个多余软件了。。。于是直接下载音乐吧，就不要软件了，用一个Python写的爬虫自己播放音乐，自己下载。（喜欢音乐请支持正版（￣。。￣）。。。）</p>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><ul>
<li>Python3.6 <a href="https://www.python.org/" target="_blank" rel="external">点击下载Python并自行安装</a><br>  所需格外的模块：<ul>
<li>BeautifulSoup4</li>
<li>selenium</li>
<li>requests</li>
<li>Jupyter Notebook 关于win10安装jupyter有些坑，<a href="http://shic.top/2017/12/23/%E9%9A%8F%E7%AC%94%EF%BC%9A%E5%85%A5%E9%97%A8%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/">详情请点击</a></li>
</ul>
</li>
<li>Windows 10 电脑，<a href="https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/" target="_blank" rel="external">点击下载浏览器驱动</a></li>
</ul>
<h3 id="爬虫脚本"><a href="#爬虫脚本" class="headerlink" title="爬虫脚本"></a>爬虫脚本</h3><p>去一下国内某知名音乐网站看看，让脚本自己播放音乐并下载到本地<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># 为爬取下载做好准备工作</div><div class="line">from selenium import webdriver</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import requests</div><div class="line">import time</div><div class="line">import os</div><div class="line">from selenium.webdriver.chrome.options import Options</div><div class="line"></div><div class="line">os.makedirs(&apos;./download_music/&apos;, exist_ok=True) # 下载文件夹就在该脚本所在的目录</div><div class="line">base_url = &quot;http://www.kugou.com/yy/html/search.html#searchType=song&amp;searchKeyWord=&quot;</div></pre></td></tr></table></figure></p>
<p>在代码里模拟输入，你就当作是一个可视化的窗口输入就行了，这里分享一下核心代码就行了嘻嘻~<br>注意：</p>
<ul>
<li>请根据自己存放的浏览器驱动MicrosoftWebDriver.exe更改下面代码第五行的位置。</li>
<li>如果网络不好，适当调大一些下面代码中的time.sleep(x)的x的数值。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"># 搜索-爬取-下载</div><div class="line">search = &quot;烈酒 屈原&quot;  #模拟输入要搜索的歌曲</div><div class="line">url = base_url+search</div><div class="line"></div><div class="line">driver = webdriver.Edge(executable_path=&quot;.\MicrosoftWebDriver.exe&quot;,)</div><div class="line">driver.get(url)</div><div class="line">time.sleep(2)</div><div class="line"></div><div class="line"># 获取搜索列表里的第一首单曲的名字，这样就实现了模糊匹配了</div><div class="line">html_search = driver.page_source</div><div class="line">soup_search = BeautifulSoup(html_search, &apos;lxml&apos;)</div><div class="line">music_a_table = soup_search.find(&apos;a&apos;,&#123;&quot;class&quot;:&quot;song_name&quot;&#125;)</div><div class="line">music_name = music_a_table[&apos;title&apos;] # 酷狗中的单曲名字是在该音乐a标签里的title属性值里的</div><div class="line"></div><div class="line">print(music_name)</div><div class="line"># 这里还有模糊匹配，下面代码是匹配含有次字符串的链接</div><div class="line"># driver.find_element_by_partial_link_text(music_name).click()</div><div class="line">driver.find_element_by_link_text(music_name).click() #当然这里就没必要模糊匹配了</div><div class="line"></div><div class="line"># 获取浏览器窗口句柄</div><div class="line">handles = driver.window_handles</div><div class="line"># print(handles,type(handles))</div><div class="line">driver.switch_to_window(handles[-1])</div><div class="line"></div><div class="line">#之前一直找不到链接，还是要让爬虫缓几秒。不知为何，page_source应该都下载好了的。。</div><div class="line">time.sleep(2)</div><div class="line"></div><div class="line">html = driver.page_source</div><div class="line">soup = BeautifulSoup(html, &apos;lxml&apos;)</div><div class="line">audio = soup.find(&apos;audio&apos;,&#123;&quot;class&quot;:&quot;music&quot;&#125;)  #只有一个audio标签，且该网站里的audio类型是music</div><div class="line">music_src = audio[&apos;src&apos;]</div><div class="line"></div><div class="line">r = requests.get(music_src,stream=True)</div><div class="line">image_name = music_name+&apos;.&apos;+music_src.split(&apos;.&apos;)[-1]</div><div class="line">with open(&apos;./download_music/%s&apos; % image_name, &apos;wb&apos;) as f:</div><div class="line">    for chunk in r.iter_content(chunk_size = 512):</div><div class="line">        f.write(chunk)</div><div class="line">    print(&apos;已保存文件： %s&apos; % image_name)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;喜欢音乐请支持正版，代码仅作学习交流。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://shic.top/categories/Python/"/>
    
    
      <category term="Python" scheme="http://shic.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>代码本地管理之Git</title>
    <link href="http://shic.top/Python/Git/use_git/"/>
    <id>http://shic.top/Python/Git/use_git/</id>
    <published>2018-02-20T16:00:00.000Z</published>
    <updated>2018-02-21T09:17:28.337Z</updated>
    
    <content type="html"><![CDATA[<p>是时候放弃手动的管理版本的方式了，你需要Git<br><a id="more"></a></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ul>
<li>首先将Git安装到自己的电脑，<a href="https://git-scm.com/" target="_blank" rel="external">Git官网</a></li>
<li>配置好自己的Git</li>
</ul>
<h3 id="在编译器中使用Git版本管理"><a href="#在编译器中使用Git版本管理" class="headerlink" title="在编译器中使用Git版本管理"></a>在编译器中使用Git版本管理</h3><p><strong>以Pycharm为例的操作详情图文版，迁移你的文档至git版本管理</strong></p>
<center><img src="http://res.cloudinary.com/shi1996/image/upload/v1519203905/git0_mxeug2.png" alt=""></center><br>Pycharm中不会显示.git的文件夹，但实际上是存在的，我们这个仓库是创建成功的<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1519204290/git1_syhypy.png" alt=""></center><br>然后我们向版本库中添加一个文件，在提交（附带上更改的信息）再查看一下<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1519204342/git2_rfm3bq.png" alt=""></center><br>当我们把一些文件提交到git仓库中后，文件名的颜色又变正常了<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1519204408/git3_wwkvpv.png" alt=""></center><br>当我们更改了一个文件里的内容后，再命令行中查看<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1519204438/git4_wm4nqf.png" alt=""></center><br>然后提交这个更改（注意-am和-m的差别）<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1519204475/git5_sox5b3.png" alt=""></center><br>然后我们回到改之前 git checkout <em>*</em> – file_name<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1519204551/git6_luuhas.png" alt=""></center>

<p>发现文件的确改变了，这对于我们每一个更改了很大一部分代码后再想回到过去很有效率，这样版本控制的作用就体现出来了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;是时候放弃手动的管理版本的方式了，你需要Git&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://shic.top/categories/Python/"/>
    
      <category term="Git" scheme="http://shic.top/categories/Python/Git/"/>
    
    
      <category term="Python" scheme="http://shic.top/tags/Python/"/>
    
      <category term="Git" scheme="http://shic.top/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>2017 念念不忘 必有回响</title>
    <link href="http://shic.top/Others/2017recollect/"/>
    <id>http://shic.top/Others/2017recollect/</id>
    <published>2018-02-14T16:00:00.000Z</published>
    <updated>2018-02-16T05:33:18.595Z</updated>
    
    <content type="html"><![CDATA[<p>现在总给我一种感觉，时间越走越快，还没来得及封装成记忆，它又带来了全新的一年。。。<br><a id="more"></a></p>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=22712173&auto=1&height=66"></iframe>

<p><br><br>我在看暴走大事件的时候，有一期提到了念念不忘，必有回响，深深地触动了我的神经，因为今年所经历了比往年都多得多的事情，也做了很多疯狂而有趣的事。<br><br><br>记忆是自己的，但照片却能让我们产生共鸣，2017是忙碌的一年,累的对自己喜欢的专业产生了怀疑，但也高兴过，因为自己又突破了一个难关，取得了进步。<br>每天关闭电脑前，都仿佛电脑也如释重负，有时候真的很累，但希望自己能够坚持。</p>
<center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518684587/ram_ftexej.png" alt=""></center><br><br><br>当然还上了很多玄学课，你懂我也懂，我们做的表情包也是让人哭笑不得<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518684925/-97d4e3b0c0f457b_1_1_1_ws59no.jpg" alt=""></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518684926/-97d4e3b0c0f457b_1_lirsnh.jpg" alt=""></center><br><br><br>这一年并非一帆风顺，除了那些被学渣吐槽的玄学，还有我们被拘束的视野，我们在百忙之中抽出时间去做这件事，终于重新与世界连通时的喜悦是这样的：<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518685149/00_k0ldzf.png" alt=""></center><br><br><br>时间真的过得快，紧紧张张向前行，谈笑风生又一年。<br>你永远不知下一次被灌醉是哪种方式<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518685382/mmexport1518682339349_dwzgfq.jpg" alt=""></center><br>也不知那几爷子是举的什么东西和你碰杯 /笑哭<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518685402/IMG_20171017_205913_bnxo0s.jpg" alt=""></center><br><br><br>有过了一年，寝室从原来的7人变到了4人，再到了5人，我们紧跟时代的步伐<center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518686178/mmexport1518685960856_dye4dy.jpg" alt=""></center><br>也不忘记搞笑和滑稽<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518686186/%E5%AF%9D%E5%AE%A4_zs6azi.png" alt=""></center><br>一起煮饭吃的感觉也是不能更棒，405还是那么欢乐，那么好～<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518686228/IMG_20170908_182643_fv8fir.jpg" alt=""></center><br><br><br>我们今年经常说的词语有：<strong>载客漂移</strong>，尽管我们的山地车都没有后座，但依然不减这词所带来的欢乐，我们就连骑车都是这么有趣。<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518686692/%E8%BD%BD%E5%AE%A2%E6%BC%82%E7%A7%BB_mnp0jk.png" alt=""></center><br>骑了4000多公里还没保养过的自行车车，拉紧刹车的时候只是有点烧胎，一样有活力<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518686993/IMG_20171210_115647_wbwtab.jpg" alt=""></center><br>感觉要成老司机的我们，唯一一次胆战心惊，可能就是”酒”驾吧，不知这啤酒载在车上会不会被查<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518686785/mmexport1518685880207_xliajx.jpg" alt=""></center><br>我们骑车放音乐，我才不怕被认成”街娃儿”，有长这么帅的？（开玩笑别当真嘿嘿嘿）<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518687188/IMG_20170916_103051_ogh9ue.jpg" alt=""></center><br>几十公里的山路到了一个古镇，看见的猫也是很可爱<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518687398/%E6%9C%9B%E9%B1%BC%E7%8C%AB1_uz5sjl.jpg" alt=""></center><br>但这只猫却是这样拍摄的，哈哈<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518687400/%E6%9C%9B%E9%B1%BC%E7%8C%AB0_nvevdf.jpg" alt=""></center><br>我们骑了半天骑了100+公里的山路，来到了洪雅的雅女湖，这才第一次看过这么清澈的水，虽然很累，但眼前的景观真的值<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518690234/IMG_20170916_152225_erar2x.jpg" alt=""></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518690257/IMG_20170916_211335_fedyim.jpg" alt=""></center><br>当然也只有我们几个男生愿意去冒险骑这么远的山路了，有时候只是为了爽一把，去飙个山路。<br>其余有女生的时候我们都走得很近，不过也玩儿得很开心，骑车也会成为大学中不可磨灭的记忆<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518691049/%E5%A4%9A%E4%BA%BA%E9%AA%91%E8%BD%A6_q5ul0u.jpg" alt=""></center><br>当然不得不说的就是这哥子锁车真的是厉害了。。。<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518691224/%E9%94%81%E8%BD%A6_xberyp.jpg" alt=""></center><br><br><br><strong>当然除了这些疯狂的半天骑100+公里山路，还有就是半天暴走几十公里。。。。。。。山路</strong>   /笑哭<br>海拔2000m到4000m，这垂直高度2公里，我们走了30+公里，看到了异常美丽的日出和云海，这里是达瓦更扎，对面能望见四姑娘山，那天是2017年12月30日<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518697489/IMG_20171231_085338_HDR_lk25mp.jpg" alt=""></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698597/%E4%BA%91%E6%B5%B7_brztex.jpg" alt=""></center><br>山路自己走，还背上几十斤的露营装备，还好我没有高原反应，还跑下去接了同伴<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698180/%E5%B1%B1%E8%B7%AF_ikfcqr.jpg" alt=""></center><br>气压低得连面包都快爆炸了<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698489/%E9%9D%A2%E5%8C%85_e5nudx.jpg" alt=""></center><br>云海可真美丽呀~<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698662/IMG_20171231_082819_HDR_l1zbvx.jpg" alt=""></center><br>然而。。。鬼才知道我们昨晚是怎么度过的<br>烤火！！我的第一次通宵，一晚没睡，零下很多度的感觉，四川人重来没看到这么大的雪！一直怀疑自己是不是要被冻死了，我们的群名字也在后来改成了“达瓦更扎生死之交”<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698763/IMG_20171231_030813_HDR_mhqfe9.jpg" alt=""></center><br>冷得简直睡不着，但清晨的日出还是真的美丽！上上图~<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698877/IMG_20171231_071228_HDR_dkcnlm.jpg" alt=""></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698886/IMG_20171231_072201_HHT_pyn5ua.jpg" alt=""></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698890/IMG_20171231_072841_HHT_amhbp2.jpg" alt=""></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698886/IMG_20171231_072201_HHT_pyn5ua.jpg" alt=""></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518698902/IMG_20171231_073334_HDR_ea0pmj.jpg" alt=""></center><br>当然最有意义的是，我在山上和几个生死之交的小伙伴一起，度过了我21岁的生日，那里没有信号，连电话也没有打一个回家，就在有一点点信号的时候给一位朋友传了一张照片~<br><br><br><strong>疯狂？激情？还有呢，就是生活中的那些</strong><br>天冷了，流浪狗在草丛中的样子让人怜悯<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518699805/IMG_20171120_163348_scgr5l.jpg" alt=""></center><br>因为我颞颌关节伤势的好转，今年我也是特别能吃，我在近4年里第一次长胖，还长了5斤多，不枉费我加餐吃了这么多嘻嘻~<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518700293/IMG_20171206_122057_vzrx8b.jpg" alt=""></center><br>当然还有回忆的童年而买的悠悠球，耍了几次就放弃了，实在是太难了<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518700543/%E6%82%A0%E6%82%A0%E7%90%83_jvsp1y.jpg" alt=""></center><br>我和老崔都不想喝酒多吃东西，于是乎我们的酒杯就只有这么大<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518700661/%E9%85%92_uolvdj.jpg" alt=""></center><br>因为右手有很重的伤，我也不知多久能好，于是便改用了左手打羽毛球，每周六都坚持去体育馆，一个人成长，现在左手也是相当灵活<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518700844/Screenshot_2018-02-15-17-10-19-946_com.tencent.mm_hyz2ai.png" alt=""></center><br><strong>其实平时收集这些记忆很少，有太多太多的事都不能用一些工具来表达，但有时候，他们的话，却真的真的触动着我</strong><br>洲洋是我高中时期最最最好的伙伴，给了我很多帮助。希望我们的友谊永恒<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518701494/IMG_20180215_212232_u7h6iw.png" alt=""></center><br>磊哥给我说“做自己喜欢的事！”，我一定会的，未来加油！我会一直想着自己喜欢的方向前进<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1518701536/IMG_20180215_212314_ybe68o.png" alt=""></center>


<p><strong>或许还真的没有更多的图了，但以前每一刻的记忆却依然存在</strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;现在总给我一种感觉，时间越走越快，还没来得及封装成记忆，它又带来了全新的一年。。。&lt;br&gt;
    
    </summary>
    
      <category term="Others" scheme="http://shic.top/categories/Others/"/>
    
    
      <category term="Others" scheme="http://shic.top/tags/Others/"/>
    
  </entry>
  
  <entry>
    <title>机器学习识别方圆(1)</title>
    <link href="http://shic.top/Deep-Learning/ML_csv_pretreatment_data/"/>
    <id>http://shic.top/Deep-Learning/ML_csv_pretreatment_data/</id>
    <published>2018-02-05T16:00:10.000Z</published>
    <updated>2019-05-11T12:54:42.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="想从这个很简单的二分类问题实践机器学习吗？"><a href="#想从这个很简单的二分类问题实践机器学习吗？" class="headerlink" title="想从这个很简单的二分类问题实践机器学习吗？"></a>想从这个很简单的二分类问题实践机器学习吗？</h2><p><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1517904003/fang_yuan_qx0ukp.jpg" alt=""></center><br><a id="more"></a></p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>可能绝大多数机器学习的入门者都用过MNIST数据集，然后第二步自己做实践的时候就卡住了，这一步中往往时相当费时间的，也是学习过程中一个困难的地方。这次博主将分享一个用机器学习来分辨图片的简单例子，带你走进实践，有利于对其工作原理的理解，便于后续的学习和研究。如果你还没有了解什么时机器学习，或者一些常用的算法，那本教程不适合。</p>
<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><p>点击<a href="https://pan.baidu.com/s/1kWpzaS3" target="_blank" rel="external">下载数据集</a> 密码: vr76，<strong>请勿用作商业用途，数据集仅做学习交流</strong></p>
<h3 id="数据预查看"><a href="#数据预查看" class="headerlink" title="数据预查看"></a>数据预查看</h3><p>使用matplotlib查看数据的图片形式</p>
<ul>
<li>本次训练我们将用到一组方圆的数据集，每一单位的数据就是一个包含大量噪点的图形，图片表示方形或者圆形，如图所示：<img src="http://res.cloudinary.com/shi1996/image/upload/v1517887539/img_of_data_t4vx8s.jpg" alt=""> </li>
<li>其中用0表示圆，1表示方形。</li>
</ul>
<h3 id="数据集分析"><a href="#数据集分析" class="headerlink" title="数据集分析"></a>数据集分析</h3><p>下载数据集并解压后会得到三个csv格式的文件，这是能用Microsoft Excel打开的文件，打开发现数据并不是我们马上能够用于机器学习的数据，我们需要进行分析和预处理。</p>
<ul>
<li>该数据集包含：训练集中4000个灰度图像，预测集3550个灰度图像，其中还包含了大量的图像噪点。图像分辨率为40*40。以行向量的形式存放在csv文件里</li>
<li>打开train.csv，第一行第一个就是id，从上往下0～3999，表示一共4000个数据。id后面一个是p_0_0,一直往右是到p_39_39，一共40*40个数据，每一行中的这1600个数据。分别代表图像(40,40)矩阵中相应位置上的像素点的灰度值。每一行最后一个标签为y，值0或者1，代表该行向量的数据表示的是圆形还是方形。数据集如下所示：<img src="http://res.cloudinary.com/shi1996/image/upload/v1517894328/train_dataSet_cwxs3c.jpg" alt=""></li>
</ul>
<h3 id="预处理、准备数据"><a href="#预处理、准备数据" class="headerlink" title="预处理、准备数据"></a>预处理、准备数据</h3><p>通过对数据的分析，我们发现这些数据并不是我们能够马上拿来给神经网络来学习的数据，我们需要对数据进行一些预处理以保证我们的神经网络能够真正从数据中学习到参数，把没有用的字段删除，对某些字段进行编码等等。并划分好训练集和验证集。注：请注意数据集文件的位置，此处是和代码文件在一个文件夹中。</p>
<h4 id="去掉id字段"><a href="#去掉id字段" class="headerlink" title="去掉id字段"></a>去掉id字段</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line"># 准备数据</div><div class="line">data_set = pd.read_csv(&apos;train.csv&apos;).iloc[:,1:]</div></pre></td></tr></table></figure>
<h4 id="one-hot-热编码labels"><a href="#one-hot-热编码labels" class="headerlink" title="one_hot 热编码labels"></a>one_hot 热编码labels</h4><p>我们可以发现数据集里的y标签，用0表示的圆，用1表示的方，但是方圆却不是像0和1这样有数值之间的距离感。我们的目的是训练一个神经网络能够对图像进行分类处理，所以我们还需要对数据进行一次写处理，这里所采用的是onehot形式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def to_onehot(y):</div><div class="line">	&quot;&quot;&quot; </div><div class="line">		这是我自己定义的一个转化形式，当然还有其他的方法</div><div class="line">		这点效率略微低</div><div class="line">	&quot;&quot;&quot;</div><div class="line">    for i in range(len(y)):</div><div class="line">        if y.iloc[i,1600]==0:</div><div class="line">            y.iloc[i,1600] = &apos;y&apos;</div><div class="line">        else:</div><div class="line">            y.iloc[i,1600] = &apos;f&apos;</div><div class="line">    return pd.get_dummies(y)</div><div class="line"></div><div class="line">data_set = to_onehot(data_set)</div></pre></td></tr></table></figure>
<h4 id="打乱数据顺序"><a href="#打乱数据顺序" class="headerlink" title="打乱数据顺序"></a>打乱数据顺序</h4><p>我们通过对数据集的分析可以看出，这些数据都还是相当有规律的，差不多很小的一批数据里，方圆的比例都是各占一半。我们先打乱一下数据的顺序，有利于神经网络的学习。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 之后会用到tensorflow.float32格式的数据，这里转化成np.float32</div><div class="line">data_set = data_set.values.astype(np.float32)</div><div class="line">np.random.shuffle(data_set)</div></pre></td></tr></table></figure></p>
<h4 id="划分训练集和验证集"><a href="#划分训练集和验证集" class="headerlink" title="划分训练集和验证集"></a>划分训练集和验证集</h4><p>test.csv文件中没有正确的y标签，仅仅能用作做模型的衡量，这是一组没有正确答案的数据，我们只有划分train .csv来得到我们的训练集和验证集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sep = int(0.7*len(data_set))</div><div class="line">train_data = data_set[:sep]</div><div class="line">test_data = data_set[sep:]</div></pre></td></tr></table></figure></p>
<h3 id="验证数据集"><a href="#验证数据集" class="headerlink" title="验证数据集"></a>验证数据集</h3><ul>
<li>可以通过多次打印得到的矩阵的维度来进行切割正确性判断，或随机打印几个数据对照真实数据，检验自己的数据预处理是否正确。</li>
<li>如果对这里的行向量理解不是很深刻，不相信这能表示图片信息的话，还可以随便找一行输出来看看是什么，图片如上面所示</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line"># 这里输出第996个图像观察一下</div><div class="line">plt.imshow(train_data.iloc[995].reshape(40,40), cmap=&apos;gray&apos;)</div><div class="line">plt.title(&apos;%i&apos; % train_labels.iloc[995,0]); plt.show()</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;想从这个很简单的二分类问题实践机器学习吗？&quot;&gt;&lt;a href=&quot;#想从这个很简单的二分类问题实践机器学习吗？&quot; class=&quot;headerlink&quot; title=&quot;想从这个很简单的二分类问题实践机器学习吗？&quot;&gt;&lt;/a&gt;想从这个很简单的二分类问题实践机器学习吗？&lt;/h2&gt;&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://res.cloudinary.com/shi1996/image/upload/v1517904003/fang_yuan_qx0ukp.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习识别方圆(2)</title>
    <link href="http://shic.top/Deep-Learning/ML_csv_build_model/"/>
    <id>http://shic.top/Deep-Learning/ML_csv_build_model/</id>
    <published>2018-02-05T16:00:09.000Z</published>
    <updated>2019-05-11T12:55:51.230Z</updated>
    
    <content type="html"><![CDATA[<h2 id="搭建模型-CNN"><a href="#搭建模型-CNN" class="headerlink" title="搭建模型-CNN"></a>搭建模型-CNN</h2><p>本次先用CNN来搭建神经网络，实现图像的识别</p>
<p><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1517897894/cnn_h1tuhm.jpg" alt=""></center><br><a id="more"></a><br><br></p>
<h3 id="参数、超参数"><a href="#参数、超参数" class="headerlink" title="参数、超参数"></a>参数、超参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">batch_size = 50  # 一批取50个数据</div><div class="line">lr = 0.5         # learning rate</div></pre></td></tr></table></figure>
<h3 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line"># 创建模型</div><div class="line">tf_input = tf.placeholder(tf.float32,[None,1602])</div><div class="line">tf_x = tf_input[:,:1600]</div><div class="line">x_img = tf.reshape(tf_x,[-1,40,40,1])</div><div class="line">tf_y = tf_input[:,1600:]            # onehot</div><div class="line"></div><div class="line">## CNN</div><div class="line">conv1 = tf.layers.conv2d(           # shape:(40,40,1)</div><div class="line">        inputs = x_img,</div><div class="line">        filters = 10,               # 图像卷积后的深度</div><div class="line">        kernel_size = 5,            # 扫描核5*5大小</div><div class="line">        padding = &apos;same&apos;,</div><div class="line">        activity_regularizer = tf.nn.relu</div><div class="line">)                                   # shape:(40,40,10)</div><div class="line">pool1 = tf.layers.max_pooling2d(</div><div class="line">        inputs = conv1,</div><div class="line">        pool_size = [2,2],</div><div class="line">        strides = 2</div><div class="line">)                                   # shape:(20,20,10)</div><div class="line">conv2 = tf.layers.conv2d(</div><div class="line">        inputs = pool1,</div><div class="line">        filters = 20,</div><div class="line">        kernel_size = 5,</div><div class="line">        padding = &apos;same&apos;,</div><div class="line">        activity_regularizer = tf.nn.relu</div><div class="line">)                                   # shape:(20,20,20)</div><div class="line">pool2 = tf.layers.max_pooling2d(</div><div class="line">        inputs = conv2,</div><div class="line">        pool_size = [2,2],</div><div class="line">        strides = 2</div><div class="line">)                                   # shape:(10,10,20)</div><div class="line"></div><div class="line">shape = pool2.get_shape().as_list() # (ง •_•)ง</div><div class="line">flat_data = tf.reshape(pool2,[-1,shape[1]*shape[2]*shape[3]])</div><div class="line">output = tf.layers.dense(flat_data,2) #用于全连接层，二分类0或者1。</div></pre></td></tr></table></figure>
<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>此处我们选择的是交叉熵损失函数，需要注意的是这里的tf_yone_hot形式的，来自数据集的y，output就是模型的预测值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">loss = tf.losses.softmax_cross_entropy(</div><div class="line">		onehot_labels = tf_y,</div><div class="line">		logits = output</div><div class="line">)</div></pre></td></tr></table></figure></p>
<h3 id="选择优化方法训练"><a href="#选择优化方法训练" class="headerlink" title="选择优化方法训练"></a>选择优化方法训练</h3><p>此处我们选择的是AdamOptimizer，当然也可以尝试其他优化器。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_op = tf.train.AdamOptimizer(lr).minimize(loss)</div></pre></td></tr></table></figure></p>
<h3 id="计算模型精度"><a href="#计算模型精度" class="headerlink" title="计算模型精度"></a>计算模型精度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 用于测试该模型的识别精确度</div><div class="line">accuracy = tf.metrics.accuracy(</div><div class="line">        labels = tf.argmax(tf_y,axis = 1),</div><div class="line">        predictions = tf.argmax(output,axis = 1)</div><div class="line">)[1]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;搭建模型-CNN&quot;&gt;&lt;a href=&quot;#搭建模型-CNN&quot; class=&quot;headerlink&quot; title=&quot;搭建模型-CNN&quot;&gt;&lt;/a&gt;搭建模型-CNN&lt;/h2&gt;&lt;p&gt;本次先用CNN来搭建神经网络，实现图像的识别&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://res.cloudinary.com/shi1996/image/upload/v1517897894/cnn_h1tuhm.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习识别方圆(3)</title>
    <link href="http://shic.top/Deep-Learning/ML_csv_Run/"/>
    <id>http://shic.top/Deep-Learning/ML_csv_Run/</id>
    <published>2018-02-05T16:00:08.000Z</published>
    <updated>2019-05-11T12:55:47.444Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Run-Your-Neural-Network"><a href="#Run-Your-Neural-Network" class="headerlink" title="Run Your Neural Network"></a>Run Your Neural Network</h2><p><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1517903727/neural-net-head_pf5ixb.jpg" alt=""></center><br><a id="more"></a></p>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p><strong>这一步是相当重要的</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line"># 初始化全局和本地变量</div><div class="line">init_op = tf.group(</div><div class="line">        tf.global_variables_initializer(), </div><div class="line">        tf.local_variables_initializer())</div><div class="line">sess.run(init_op)</div></pre></td></tr></table></figure></p>
<h3 id="训练并实时检验"><a href="#训练并实时检验" class="headerlink" title="训练并实时检验"></a>训练并实时检验</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># 训练</div><div class="line">for step in range(1201):</div><div class="line">    batch_index = np.random.randint(len(train_data), size = batch_size)</div><div class="line">    sess.run(train_op,&#123;tf_input:train_data[batch_index]&#125;)</div><div class="line">    if step % 50 ==0:</div><div class="line">        acc_, loss_ = sess.run([accuracy, loss], &#123;tf_input: test_data&#125;)</div><div class="line">        print(&quot;Step: %i&quot; % step, &quot;| Accurate: %.8f&quot; % acc_, &quot;| Loss: %.2f&quot; % loss_ )</div><div class="line">	</div><div class="line">	# 检验一下是否输出正常</div><div class="line">    if step == 1200:</div><div class="line">        predictions_2 = tf.argmax(output,axis = 1)</div><div class="line">        test_data_ = test_data[:50]</div><div class="line">        acc_,predictions_2_ ,output_= sess.run([accuracy,predictions_2,output], feed_dict = &#123;tf_input: test_data_&#125;)</div><div class="line">        print(&apos;predictions_2:&apos;, predictions_2_,&apos;output:&apos;,output_)</div><div class="line">        print(&apos;accuracy:&apos;,acc_)</div></pre></td></tr></table></figure>
<p>在经过很短的训练后，还是能达到约99%的精确度（step = 1200，accuracy = 92.5%，图像噪点较多）</p>
<h3 id="优化及总结"><a href="#优化及总结" class="headerlink" title="优化及总结"></a>优化及总结</h3><ul>
<li><p>我们首先用的是0.05的学习率进行学习，在达到98%的时候就比较难上到99%，当然经过更多的训练还是可以。那么此处就可以更改一下learning rate，换一个更小的值来试试。</p>
</li>
<li><p>当然我们也可以把代码迁移到GPU版的Tensorflow上。这里我训练的时候我的GTX950M出现了爆显存的错误。。当然应该可以通过优化模型，更改每一批的数据来解决。暂时我还没做这一块。<strong>但需要注意的是，目前Tenssorflow的CPU版和GPU版是不能同时安装的</strong>，之后有时间再分享一期有关于这方面的优化加速方法吧。</p>
</li>
<li><p>如果是没有GPU，也可以通过源代码安装Tensorflow获取对于AVX，AVX2指令集的支持。当然关于这一条我实验了好几天。因为博主的电脑是Windows 10操作系统的，也没有安装Linux，所以官方就不支持源代码安装到Windows机器上。如果你也用的是Windows电脑，这里给出花了好几天来实验得到经验和信息：</p>
<ul>
<li>Tensorflow官方不支持源代码安装到WIndows，所以只有自己操作，需要用到cmake，swigwin，visual studio三个工具，其中vs2015只能编译出支持AVX指令集的wheel文件，<strong>vs2017才能编译出支持AVX和AVX2的</strong></li>
<li>编译时只支持Python3.5-amd64，不支持Python3.6，用3.6的python会编译出错，最终无法得到wheel文件</li>
<li>编译时会从git下载一些数据文件，网速非常非常的慢，我挂了六小时才下载完，一直停在下载没有变是正常现象。建议启用VPN全局代理（ssr需要选择代理规则为全局代理，再选PAC才行）</li>
</ul>
</li>
</ul>
<ul>
<li>我们也可以用到一些迁移学习的思想，我们可以先对模型用一个较大的learning rate进行学习，然后到一定程度后就把学习到的神经网络的参数保存下来，下一次换一个小点的learning rate，把保存的参数读取处理。在此基础上再进行学习。关于这一点，博主之后有时间的话也会写一个帖子分享一下。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Run-Your-Neural-Network&quot;&gt;&lt;a href=&quot;#Run-Your-Neural-Network&quot; class=&quot;headerlink&quot; title=&quot;Run Your Neural Network&quot;&gt;&lt;/a&gt;Run Your Neural Network&lt;/h2&gt;&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://res.cloudinary.com/shi1996/image/upload/v1517903727/neural-net-head_pf5ixb.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习识别方圆(4)</title>
    <link href="http://shic.top/Deep-Learning/ML_csv_rnn/"/>
    <id>http://shic.top/Deep-Learning/ML_csv_rnn/</id>
    <published>2018-02-05T16:00:07.000Z</published>
    <updated>2019-05-11T12:50:17.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RNN照样识别你是方是圆"><a href="#RNN照样识别你是方是圆" class="headerlink" title="RNN照样识别你是方是圆"></a>RNN照样识别你是方是圆</h2><p><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1517922013/rnn_c77e5f.jpg" alt=""></center><br><a id="more"></a></p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>前面用CNN实践了一下，其实用RNN也是可以识别图像的。只是说存在这在于更加大型的模型里的好坏问题，在这个小数据集上，RNN同样也是可以使用做分类处理的，而且这样也能加深理解。<br><br><br>用下图解释一下RNN处理图像时的输入<br><img src="http://res.cloudinary.com/shi1996/image/upload/v1517923518/rnn__s8xouo.jpg" alt=""><br>难点还是在处理数据，调整模型使之与数据维度匹配</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">import pandas as pd</div><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line"># 超参数</div><div class="line">batch_size = 50</div><div class="line">time_step = 40</div><div class="line">input_size = 40</div><div class="line">lr = 0.005</div><div class="line">hidden_unit = 256</div><div class="line"></div><div class="line">def to_onehot(y):</div><div class="line">    for i in range(len(y)):</div><div class="line">        if y.iloc[i,1600]==0:</div><div class="line">            y.iloc[i,1600] = &apos;y&apos;</div><div class="line">        else:</div><div class="line">            y.iloc[i,1600] = &apos;f&apos;</div><div class="line">    return pd.get_dummies(y)</div><div class="line"></div><div class="line"># 准备数据</div><div class="line">data_set = pd.read_csv(&apos;train.csv&apos;).iloc[:,1:]</div><div class="line"># 仅把标签转化成onehot</div><div class="line">data_set = to_onehot(data_set)</div><div class="line"></div><div class="line"># 打乱数据，使数据随机</div><div class="line">data_set = data_set.values.astype(np.float32)</div><div class="line">np.random.shuffle(data_set)</div><div class="line"></div><div class="line"># 划分训练集和验证集</div><div class="line">sep = int(0.7*len(data_set))</div><div class="line">train_data = data_set[:sep]</div><div class="line">test_data = data_set[sep:]</div><div class="line"></div><div class="line"># 创建模型</div><div class="line">tf_input = tf.placeholder(tf.float32,[None,1602])</div><div class="line">tf_x = tf_input[:,:1600]</div><div class="line">x_img = tf.reshape(tf_x,[-1,time_step,input_size])</div><div class="line">tf_y = tf_input[:,1600:]            # onehot</div><div class="line"></div><div class="line">## RNN</div><div class="line">rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_unit)</div><div class="line">output_ , states = tf.nn.dynamic_rnn(</div><div class="line">        rnn_cell,               #cell</div><div class="line">        x_img,                  #input</div><div class="line">        initial_state = None,   #初始化隐藏层的状态,表示不初始化</div><div class="line">        dtype = tf.float32,     #和上面那句必须同时出现</div><div class="line">        time_major = False      #False: (batch, time_step, input); True: (time_step, batch, input)</div><div class="line">)</div><div class="line"></div><div class="line">output = tf.layers.dense(output_[:,-1,:],2) #对输出进行最后的修改</div><div class="line"></div><div class="line">loss = tf.losses.softmax_cross_entropy(onehot_labels = tf_y,logits = output)</div><div class="line">train_op = tf.train.AdamOptimizer(lr).minimize(loss)</div><div class="line"></div><div class="line"># 计算精度</div><div class="line">accuracy = tf.metrics.accuracy(</div><div class="line">        labels = tf.argmax(tf_y,axis = 1),</div><div class="line">        predictions = tf.argmax(output,axis = 1)</div><div class="line">)[1]</div><div class="line"></div><div class="line"># 重要步骤！！初始化</div><div class="line">sess = tf.Session()</div><div class="line">#初始化全局和本地变量</div><div class="line">init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())</div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"># 训练</div><div class="line">print(&quot;RNN模型，learning rate：&quot;,lr)</div><div class="line">for step in range(1201):</div><div class="line">    batch_index = np.random.randint(len(train_data), size = batch_size)</div><div class="line">    sess.run(train_op,&#123;tf_input:train_data[batch_index]&#125;)</div><div class="line">    if step % 50 ==0:</div><div class="line">        acc_, loss_ = sess.run([accuracy, loss], &#123;tf_input: test_data&#125;)</div><div class="line">        print(&quot;Step: %i&quot; % step, &quot;| Accurate: %.8f&quot; % acc_, &quot;| Loss: %.2f&quot; % loss_ )</div><div class="line"></div><div class="line">    if step == 1200:</div><div class="line">        predictions_2 = tf.argmax(output,axis = 1)</div><div class="line">        test_data_ = test_data[:50]</div><div class="line">        acc_,predictions_2_ ,output_= sess.run([accuracy,predictions_2,output], feed_dict = &#123;tf_input: test_data_&#125;)</div><div class="line">        print(&apos;predictions_2:&apos;, predictions_2_,&apos;output:&apos;,output_)</div><div class="line">        print(&apos;accuracy:&apos;,acc_)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;RNN照样识别你是方是圆&quot;&gt;&lt;a href=&quot;#RNN照样识别你是方是圆&quot; class=&quot;headerlink&quot; title=&quot;RNN照样识别你是方是圆&quot;&gt;&lt;/a&gt;RNN照样识别你是方是圆&lt;/h2&gt;&lt;p&gt;&lt;center&gt;&lt;img src=&quot;http://res.cloudinary.com/shi1996/image/upload/v1517922013/rnn_c77e5f.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://shic.top/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="http://shic.top/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>随笔：入门所遇到的坑</title>
    <link href="http://shic.top/TensorFlow/%E9%9A%8F%E7%AC%94%EF%BC%9A%E5%85%A5%E9%97%A8%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>http://shic.top/TensorFlow/随笔：入门所遇到的坑/</id>
    <published>2017-12-22T16:00:04.000Z</published>
    <updated>2018-02-06T13:54:46.624Z</updated>
    
    <content type="html"><![CDATA[<h2 id="真的是随笔，不信算了"><a href="#真的是随笔，不信算了" class="headerlink" title="真的是随笔，不信算了"></a><center>真的是随笔，不信算了</center></h2><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>Windows 10 64Bit 1709Pro</li>
<li>TensorFlow1.4.0</li>
<li>Python 3.6.3</li>
<li></li>
<li></li>
<li><a id="more"></a>
</li>
</ul>
<ol>
<li><p>Jupyter安装出问题，不能打开jupyter notebook<br>答：安装提示是找不到markupsafe模块，但是pip list一查看，的确是安装了的，有说法是在Windows终端上的cmd编码有问题，于是就可以换一个终端来使用，比如说在git bash里，卸载markupsafe并重新安装，然后就能顺利打开，运行jupyter notebook浏览器就会自动打开。</p>
<center><img src="https://res.cloudinary.com/shi1996/image/upload/v1510803291/gitbash_r04neu.png" alt="gitbash"></center><br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1510803295/jupyter_ibqqjg.png" alt="jupyter"></center><br>但大家会发现这进入的是当前用户的一个文件夹，推荐是在python的模块文件夹下，右键点击git bash here，就能用jupyter打开该文件夹了<br><center><img src="http://res.cloudinary.com/shi1996/image/upload/v1510803930/gitbashhere_zebgzz.png" alt="gitbashhere"></center>
</li>
<li><p><a href="https://www.tensorflow.org/install/migration" target="_blank" rel="external">过度到Tensorflow 1.0版本</a>,有些教程用的版本比较低，新版的API与旧版的差别看这</p>
</li>
<li></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;真的是随笔，不信算了&quot;&gt;&lt;a href=&quot;#真的是随笔，不信算了&quot; class=&quot;headerlink&quot; title=&quot;真的是随笔，不信算了&quot;&gt;&lt;/a&gt;&lt;center&gt;真的是随笔，不信算了&lt;/center&gt;&lt;/h2&gt;&lt;h2 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境&quot;&gt;&lt;/a&gt;环境&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Windows 10 64Bit 1709Pro&lt;/li&gt;
&lt;li&gt;TensorFlow1.4.0&lt;/li&gt;
&lt;li&gt;Python 3.6.3&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="http://shic.top/categories/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://shic.top/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>在Ubuntu里使用Python</title>
    <link href="http://shic.top/Linux/Ubuntu_python/"/>
    <id>http://shic.top/Linux/Ubuntu_python/</id>
    <published>2017-12-22T16:00:03.000Z</published>
    <updated>2017-12-28T02:26:31.137Z</updated>
    
    <content type="html"><![CDATA[<p>大家应该都听说过，现在很多Linux系统都自带有Python，Ubuntu也不例外，但是因为版本原因，我们该如何正确使用它呢<br><a id="more"></a></p>
<h3 id="检查Python"><a href="#检查Python" class="headerlink" title="检查Python"></a>检查Python</h3><ul>
<li>首先我们检查一下Python才发现，原来只是自带了Python2.7，但是这时如果再输入一下<strong>Python3</strong>的话，就会发现其实Python3也是安装了的，而且Ubuntu17.10还是自带的Python3.6.2版本的。<img src="http://res.cloudinary.com/shi1996/image/upload/v1514102549/check_python_LI_as7vbg.jpg" alt=""> 但是大家会发现pypi都没有安装。其实想想也是有道理的，这样用户就可以根据自己的需求来决定到底用哪一个版本的Python了。</li>
</ul>
<h3 id="安装Pypi和其他常用包"><a href="#安装Pypi和其他常用包" class="headerlink" title="安装Pypi和其他常用包"></a>安装Pypi和其他常用包</h3><p>以Python3.6.2为例<br><br></p>
<ol>
<li>当需要安装Python3的其他模块的时候，就会用到pip3，那么我们就   为系统安装它，输入以下命令：<br><code>sudo apt-get install python3-pip</code><br>输入y，点击回车，等待下载完成</li>
<li>安装Python的其他包就可以像在Windows中一样进行操作了。当然我们Windows通常就只安装一个版本的Python，所以我们可能会直接这样用：<br><code>pip install package_name</code><br>但是需要注意的是，这时候就会报错了，很神奇？其实这也是必然，毕竟直接输入pip install是在用Python2.7，要是想要Python3的话就得输入<strong>pip3 install</strong>，   <img src="http://res.cloudinary.com/shi1996/image/upload/v1514102538/install_pip3_podqa7.png" alt="">   这样也不会因为有两个不兼容版本的Python而不知道自己安装的模块到底属于哪一个的问题了。例如下图所示：   <img src="http://res.cloudinary.com/shi1996/image/upload/v1514102541/install_numpy_k4sxvd.png" alt=""></li>
<li><p><strong>这里提醒一点想安装Jupyter的伙伴们</strong>，这里有一个几个坑需要注意以下，我也是查了很久才知道怎么操作的，于是在这里总结以下。和Windows里的不一样，需要用到：<br><code>pip3 install jupyter-notebook</code></p>
<ul>
<li>有些电脑可能安装不上，那就检查以下jupyter，系统提示需要用到<strong>sudo apt</strong>来安装，于是跟着提示进行操作  <img src="http://res.cloudinary.com/shi1996/image/upload/v1514102537/jupyter0_vtta1y.png" alt=""></li>
<li>然后在输入jupyter发现是安装了，和Windows里的一样显示，但是输入jupyter notebook发现还是不行。  <img src="http://res.cloudinary.com/shi1996/image/upload/v1514102534/jupyter1_xncv8c.png" alt="">  </li>
<li>这个时候，再在Terminal里面输入下面的命令就行了<br><code>pip3 install jupyter-notebook</code></li>
</ul>
<p>最后在看一下大功告成的样子，完美~  <img src="http://res.cloudinary.com/shi1996/image/upload/c_scale,w_1999/v1514037548/IMG_20171222_203340_zvk3rm.jpg" alt=""></p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家应该都听说过，现在很多Linux系统都自带有Python，Ubuntu也不例外，但是因为版本原因，我们该如何正确使用它呢&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://shic.top/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://shic.top/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu安装本地软件</title>
    <link href="http://shic.top/Linux/Ubuntu_install_app/"/>
    <id>http://shic.top/Linux/Ubuntu_install_app/</id>
    <published>2017-12-22T16:00:02.000Z</published>
    <updated>2017-12-25T14:33:48.563Z</updated>
    
    <content type="html"><![CDATA[<p>安装一个软件都能报出依赖缺失的错，是不是觉得简直了。。。<br>感觉又何Java开发类似了，不停地找啥以来直到用上spring<br><a id="more"></a><br>我们一安装网易云音乐和搜狗拼音为例来进行操作，之后的软件安装都可以类似操作。</p>
<h3 id="安装网易云音乐"><a href="#安装网易云音乐" class="headerlink" title="安装网易云音乐"></a>安装网易云音乐</h3><ol>
<li>先从网上下载安装包到本地，然后在下载的文件夹中打开Terminal（右键-打开终端）,输入以下安装本地软件的命令（中括号中请替换成相应软件的名字，包括格式，例如:abc123.deb）<br><code>sudo dpkg -i [your app&#39;s name]</code><br>如下图所示，输入密码后可能会出现很多依赖缺失的情况  <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083063/163_0_mlrvmy.png" alt="0"> 那么我们就可以根据提示去做，它说未安装的软件包，我们就去安装就行了。</li>
<li>当我们输入安装软件包的指令后，他会去我们设定好的软件源去下载：<br><code>sudo apt-get install gconf-service</code><br>会发现依然有很多毛病不能正常安装，它又说没有gconf-service-backend，那是不是又要去安装它呢，万一又缺啥是不是就进入死循环了呢？  <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083049/163_1_tdvl9v.png" alt="1">  哈哈，先不急着往下安装，<strong>每一次报错都要仔细看一下提示信息嘛，一般都很有用的</strong></li>
<li>提示说有未能满足的依赖关系。请尝试不指明软件包的名字来运行apt     –fix-broken install然后给出了一行命令，那么我们就输入它试试                    看：<br><code>apt --fix-broken install</code></li>
<li>输入之后你会发现它提示权限不够，然而我们在安装系统的时候所设置        的用户就是超级用户了，也就是管理员了。然而很多同学可能就会。想到Windows下有提升用户权限的操作，于是就去搜Ubuntu提升用户。权限的操 作，结果又没有搜到于是就一直卡在这里。 <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083057/163_2_hnxtwo.png" alt="2"><br>其实要是懂一点Linux的基本命令就会知道，而且我们之前安装软件都输入了<strong>sudo</strong>的字符，sudo是linux系统管理指令，是允许系统管理员让普通用户执行一些或者全部的root命令的一个工具。简单理解可以是super user do。那么我们尝试一下下面的命令：<br><code>sudo apt --fix-broken install</code></li>
<li>输入之后你将看到一下的界面  <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083070/163_3_s7yjdj.png" alt="3">  输入y点击回车就行了，要是你不放心呢，就去输入命令检查一下，结果也是发现以前缺少的依赖都在了  <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083065/163_4_yw7xgw.png" alt="4">  那么你就可以在该软件安装包所在的文件夹里打开Terminal，使用一下命令安装你的软件了：<br><code>sudo dpkg -i [your app&#39;s name]</code><br><br></li>
</ol>
<h3 id="安装搜狗拼音"><a href="#安装搜狗拼音" class="headerlink" title="安装搜狗拼音"></a>安装搜狗拼音</h3><p>讲真，自带的中文输入法还真的不好用，于是我们换成了对中文输入更加友好的搜狗拼音。</p>
<ol>
<li>和之前的安装步骤一样，遇到问题一样的操作，你自己操作一遍就会熟练了。 <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083059/pinyin0_ifoxwf.png" alt="5">  <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083053/pinyin1_rbuxxa.png" alt="5"></li>
<li>安装好之后就如图进行设置 <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083059/pinyin3_ws8bsm.png" alt="6"> 选择键盘输入法系统为fcitx，有些电脑可能会出现下面图中定位情况，选择是或者安装就行了 <img src="http://res.cloudinary.com/shi1996/image/upload/v1514083066/pinyin2_l4utx6.png" alt="7"></li>
<li>最后我们检验一下，发现能用就大功告成啦  <img src="http://res.cloudinary.com/shi1996/image/upload/c_limit,w_1182/v1514037552/IMG_20171222_222709_kyupuh.jpg" alt="8"></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;安装一个软件都能报出依赖缺失的错，是不是觉得简直了。。。&lt;br&gt;感觉又何Java开发类似了，不停地找啥以来直到用上spring&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://shic.top/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://shic.top/tags/Linux/"/>
    
  </entry>
  
</feed>
